{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GDELT Demo Notebook\n",
        "\n",
        "This notebook demonstrates working with GDELT (Global Database of Events, Language, and Tone) data for graph analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  GCP Project: graph-demo-471710\n",
            "  BigQuery Dataset: gdelt\n",
            "  BigQuery Tables: ['gkg_partitioned', 'events_partitioned']\n",
            "  GDELT Project: gdelt-bq\n",
            "  GDELT Dataset: gdeltv2\n",
            "  GDELT Region: us\n",
            "  GCS Bucket: gdelt_graph\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables\n",
        "GCP_PROJECT_ID = \"graph-demo-471710\"  # Replace with your actual GCP project ID\n",
        "PROJECT_REGION = \"us-central1\"\n",
        "BIGQUERY_DATASET = \"gdelt\"  # Replace with your actual BigQuery dataset name\n",
        "BIGQUERY_TABLES = [\"gkg_partitioned\", \"events_partitioned\",\"eventmentions_partitioned\"]  # List of tables to copy\n",
        "GDELT_PROJECT_ID = \"gdelt-bq\"\n",
        "GDELT_DATASET = \"gdeltv2\"  \n",
        "GDELT_REGION = \"us\"\n",
        "GCS_BUCKET = \"gdelt_graph\"\n",
        "\n",
        "# Derived variables - will be generated for each table\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  GCP Project: {GCP_PROJECT_ID}\")\n",
        "print(f\"  BigQuery Dataset: {BIGQUERY_DATASET}\")\n",
        "print(f\"  BigQuery Tables: {BIGQUERY_TABLES}\")\n",
        "print(f\"  GDELT Project: {GDELT_PROJECT_ID}\")\n",
        "print(f\"  GDELT Dataset: {GDELT_DATASET}\")\n",
        "print(f\"  GDELT Region: {GDELT_REGION}\")\n",
        "print(f\"  GCS Bucket: {GCS_BUCKET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCP Authentication Setup\n",
        "import subprocess\n",
        "import os\n",
        "import shutil\n",
        "from google.auth import default\n",
        "from google.auth.exceptions import DefaultCredentialsError\n",
        "\n",
        "def setup_gcp_authentication():\n",
        "    \"\"\"Complete GCP authentication setup with error handling\"\"\"\n",
        "    print(\"üîê Setting up GCP Authentication...\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Try to use existing credentials first\n",
        "        print(\"üîç Checking for existing credentials...\")\n",
        "        try:\n",
        "            credentials, default_project = default()\n",
        "            print(f\"‚úÖ Found existing credentials for project: {default_project}\")\n",
        "            \n",
        "            # If the project matches, we're good\n",
        "            if default_project == GCP_PROJECT_ID:\n",
        "                print(f\"üéØ Project matches target project: {GCP_PROJECT_ID}\")\n",
        "                os.environ['GOOGLE_CLOUD_PROJECT'] = GCP_PROJECT_ID\n",
        "                return credentials, GCP_PROJECT_ID\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Project mismatch: {default_project} vs {GCP_PROJECT_ID}\")\n",
        "                print(\"üîÑ Will re-authenticate with correct project...\")\n",
        "        except DefaultCredentialsError:\n",
        "            print(\"‚ùå No existing credentials found\")\n",
        "            print(\"üîÑ Will authenticate from scratch...\")\n",
        "        \n",
        "        # Step 2: Clear old credentials if needed\n",
        "        print(\"üóëÔ∏è  Clearing old credentials...\")\n",
        "        adc_path = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\")\n",
        "        if os.path.exists(adc_path):\n",
        "            os.remove(adc_path)\n",
        "            print(\"‚úÖ Removed old application default credentials\")\n",
        "        \n",
        "        # Step 3: Set the correct project\n",
        "        print(f\"üéØ Setting gcloud project to: {GCP_PROJECT_ID}\")\n",
        "        result = subprocess.run(['gcloud', 'config', 'set', 'project', GCP_PROJECT_ID], \n",
        "                              capture_output=True, text=True, check=True)\n",
        "        print(\"‚úÖ Project set successfully\")\n",
        "        \n",
        "        # Step 4: Re-authenticate\n",
        "        print(\"üîÑ Re-authenticating with application default credentials...\")\n",
        "        print(\"   This will open a browser window for authentication...\")\n",
        "        \n",
        "        result = subprocess.run(['gcloud', 'auth', 'application-default', 'login'], \n",
        "                              check=True)\n",
        "        print(\"‚úÖ Re-authentication successful\")\n",
        "        \n",
        "        # Step 5: Set quota project to avoid warnings\n",
        "        print(\"üí∞ Setting quota project...\")\n",
        "        try:\n",
        "            subprocess.run(['gcloud', 'auth', 'application-default', 'set-quota-project', GCP_PROJECT_ID], \n",
        "                          capture_output=True, text=True, check=True)\n",
        "            print(\"‚úÖ Quota project set successfully\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è  Could not set quota project (this is usually fine)\")\n",
        "        \n",
        "        # Step 6: Verify the setup\n",
        "        print(\"üß™ Verifying authentication...\")\n",
        "        credentials, project = default()\n",
        "        print(f\"‚úÖ Authentication successful - Project: {project}\")\n",
        "        \n",
        "        # Set environment variable\n",
        "        os.environ['GOOGLE_CLOUD_PROJECT'] = GCP_PROJECT_ID\n",
        "        print(f\"üåç Set GOOGLE_CLOUD_PROJECT environment variable to: {GCP_PROJECT_ID}\")\n",
        "        \n",
        "        return credentials, GCP_PROJECT_ID\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Command failed: {e}\")\n",
        "        print(\"üí° Manual steps required:\")\n",
        "        print(f\"   1. gcloud config set project {GCP_PROJECT_ID}\")\n",
        "        print(\"   2. gcloud auth application-default login\")\n",
        "        print(f\"   3. gcloud auth application-default set-quota-project {GCP_PROJECT_ID}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Run authentication setup\n",
        "credentials, authenticated_project = setup_gcp_authentication()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GCP connectivity\n",
        "def test_gcp_connectivity():\n",
        "    \"\"\"Test basic connectivity to GCP services\"\"\"\n",
        "    print(\"üîç Testing GCP connectivity...\")\n",
        "    \n",
        "    # Check if authentication was successful\n",
        "    if not credentials or not authenticated_project:\n",
        "        print(\"‚ùå Authentication required - please run the authentication cell first\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ Using authenticated project: {authenticated_project}\")\n",
        "    \n",
        "    # Test 1: Test BigQuery connectivity\n",
        "    try:\n",
        "        # Use explicit credentials and project\n",
        "        client = bigquery.Client(credentials=credentials, project=authenticated_project)\n",
        "        print(f\"üîó BigQuery client created for project: {client.project}\")\n",
        "        \n",
        "        # Simple query to test connectivity\n",
        "        query = \"SELECT 1 as test_value\"\n",
        "        result = client.query(query).result()\n",
        "        for row in result:\n",
        "            print(f\"‚úÖ BigQuery connectivity successful - Test query result: {row.test_value}\")\n",
        "            break  # Only need first row\n",
        "    except Exception as e:\n",
        "        error_str = str(e)\n",
        "        if \"has been deleted\" in error_str or \"USER_PROJECT_DENIED\" in error_str:\n",
        "            print(f\"‚ùå BigQuery connectivity failed: Project mismatch detected\")\n",
        "            print(f\"   Error: {e}\")\n",
        "            print(f\"üîß This usually means your credentials are cached for a different project\")\n",
        "            print(f\"   üí° Try running the authentication cell again\")\n",
        "            print(f\"   üìã Or manually run: gcloud auth application-default login\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"‚ùå BigQuery connectivity failed: {e}\")\n",
        "            return False\n",
        "    \n",
        "    # Test 2: Test BigQuery dataset access\n",
        "    try:\n",
        "        client = bigquery.Client(credentials=credentials, project=authenticated_project)\n",
        "        dataset_ref = client.dataset(BIGQUERY_DATASET)\n",
        "        dataset = client.get_dataset(dataset_ref)\n",
        "        print(f\"‚úÖ BigQuery dataset '{BIGQUERY_DATASET}' accessible\")\n",
        "        \n",
        "        # List tables in the dataset\n",
        "        tables = list(client.list_tables(dataset_ref))\n",
        "        print(f\"üìä Found {len(tables)} tables in dataset\")\n",
        "        for table in tables[:5]:  # Show first 5 tables\n",
        "            print(f\"   - {table.table_id}\")\n",
        "        if len(tables) > 5:\n",
        "            print(f\"   ... and {len(tables) - 5} more tables\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå BigQuery dataset access failed: {e}\")\n",
        "        print(f\"   Make sure dataset '{BIGQUERY_DATASET}' exists in project '{authenticated_project}'\")\n",
        "        return False\n",
        "    \n",
        "    # Test 3: Test Cloud Storage connectivity\n",
        "    try:\n",
        "        storage_client = storage.Client(credentials=credentials, project=authenticated_project)\n",
        "        # List buckets to test connectivity\n",
        "        buckets = list(storage_client.list_buckets())\n",
        "        print(f\"‚úÖ Cloud Storage connectivity successful - Found {len(buckets)} buckets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cloud Storage connectivity failed: {e}\")\n",
        "        return False\n",
        "    \n",
        "    print(\"üéâ All GCP connectivity tests passed!\")\n",
        "    return True\n",
        "\n",
        "# Run the connectivity test\n",
        "test_gcp_connectivity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ready for GDELT analysis!\n",
        "print(\"üéâ Setup complete! Ready to work with GDELT data.\")\n",
        "print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "print(\"üöÄ You can now run queries against your GDELT data!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List datasets in the GDELT_PROJECT_ID project\n",
        "def list_gdelt_datasets():\n",
        "    \"\"\"List all datasets in the GDELT_PROJECT_ID project\"\"\"\n",
        "    print(f\"üîç Listing datasets in GDELT project: {GDELT_PROJECT_ID}\")\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client for the GDELT project\n",
        "        gdelt_client = bigquery.Client(project=GDELT_PROJECT_ID)\n",
        "        print(f\"‚úÖ Connected to GDELT project: {gdelt_client.project}\")\n",
        "        \n",
        "        # List all datasets in the project\n",
        "        datasets = list(gdelt_client.list_datasets())\n",
        "        \n",
        "        if not datasets:\n",
        "            print(\"üì≠ No datasets found in the GDELT project\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"üìä Found {len(datasets)} datasets in {GDELT_PROJECT_ID}:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        dataset_info = []\n",
        "        for dataset in datasets:\n",
        "            # Get dataset details\n",
        "            dataset_ref = gdelt_client.dataset(dataset.dataset_id)\n",
        "            full_dataset = gdelt_client.get_dataset(dataset_ref)\n",
        "            \n",
        "            # Count tables in the dataset\n",
        "            tables = list(gdelt_client.list_tables(dataset_ref))\n",
        "            \n",
        "            info = {\n",
        "                'dataset_id': dataset.dataset_id,\n",
        "                'description': full_dataset.description or 'No description',\n",
        "                'created': full_dataset.created,\n",
        "                'modified': full_dataset.modified,\n",
        "                'location': full_dataset.location,\n",
        "                'table_count': len(tables)\n",
        "            }\n",
        "            dataset_info.append(info)\n",
        "            \n",
        "            print(f\"üìÅ Dataset: {dataset.dataset_id}\")\n",
        "            print(f\"   Description: {info['description']}\")\n",
        "            print(f\"   Created: {info['created']}\")\n",
        "            print(f\"   Modified: {info['modified']}\")\n",
        "            print(f\"   Location: {info['location']}\")\n",
        "            print(f\"   Tables: {info['table_count']}\")\n",
        "            \n",
        "            # Show first few tables if any\n",
        "            if tables:\n",
        "                print(f\"   Sample tables:\")\n",
        "                for table in tables[:5]:\n",
        "                    print(f\"     - {table.table_id}\")\n",
        "                if len(tables) > 5:\n",
        "                    print(f\"     ... and {len(tables) - 5} more\")\n",
        "            print()\n",
        "        \n",
        "        return dataset_info\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error listing datasets: {e}\")\n",
        "        return []\n",
        "\n",
        "# Run the function to list datasets\n",
        "gdelt_datasets = list_gdelt_datasets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Region GDELT Data Copy Function\n",
        "\n",
        "This function efficiently copies GDELT data from the US region to your local US-CENTRAL1 region using a smart multi-step approach:\n",
        "\n",
        "### üéØ **Purpose**\n",
        "- Copies GDELT data for a specific date (September 11, 2025) from the public GDELT dataset\n",
        "- Handles cross-region data transfer from US region to US-CENTRAL1 region\n",
        "- Optimizes for cost and speed with intelligent caching\n",
        "\n",
        "### üîÑ **Process Flow**\n",
        "1. **Destination Check**: Verifies if target table already exists (skips if data present)\n",
        "2. **Dataset Setup**: Creates required datasets in both US and US-CENTRAL1 regions\n",
        "3. **Temporary Table Check**: Checks if temp table exists in US region (reuses if available)\n",
        "4. **Data Query**: Queries GDELT data and saves to temporary table in US region\n",
        "5. **Cross-Region Copy**: Copies data from US region to US-CENTRAL1 region\n",
        "6. **Cleanup**: Removes temporary table and verifies final data\n",
        "\n",
        "### ‚ö° **Optimizations**\n",
        "- **Smart Caching**: Skips expensive operations if data already exists\n",
        "- **Cost Efficient**: Reuses temporary tables when possible\n",
        "- **Error Resilient**: Handles various BigQuery errors gracefully\n",
        "- **Progress Tracking**: Detailed logging throughout the process\n",
        "\n",
        "### üìä **Output**\n",
        "- Creates table: `{GCP_PROJECT_ID}.gdelt.gkg_partitioned` in US-CENTRAL1 region\n",
        "- Shows row counts and verification details\n",
        "- Provides troubleshooting tips if errors occur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Starting cross-region GDELT data copy...\n",
            " Target date: September 11, 2025\n",
            " Source: gdelt-bq.gdeltv2.gkg_partitioned (US region)\n",
            " Destination: graph-demo-471710.gdelt.gkg_partitioned (US-CENTRAL1 region)\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ BigQuery client created\n",
            "üîç Checking if destination table already exists...\n",
            "üìù Destination table doesn't exist, proceeding with data copy...\n",
            " Checking if dataset 'gdelt' exists...\n",
            "‚úÖ Dataset 'gdelt' already exists\n",
            "üìù Creating dataset in US region for temporary table...\n",
            "‚úÖ Dataset 'gdelt_us' already exists in US region\n",
            "üîç Checking if temporary table already exists...\n",
            "üìä Temporary table doesn't exist, querying GDELT data and saving to temporary table...\n",
            "üìä Executing query...\n",
            "üîç Query: \n",
            "                SELECT *\n",
            "                FROM `gdelt-bq.gdeltv2.gkg_partitioned`\n",
            "                WHERE _PARTITIONTIME = TIMESTAMP('2025-09-11')\n",
            "                \n",
            "üéØ Destination: graph-demo-471710.gdelt_us.temp_gkg_partitioned\n",
            "‚è≥ Query job started: cc412846-c5a2-44b4-a65b-15f4d18b6b52\n",
            "‚è≥ Waiting for query to complete...\n",
            "‚úÖ Data copied to temporary table in US region\n",
            "üîÑ Copying data from US region to US-CENTRAL1 region...\n",
            "‚è≥ Copy job started: 3a03b560-65f4-4190-be28-346b3c2da721\n",
            "‚è≥ Waiting for copy to complete...\n",
            "‚úÖ Data copied to US-CENTRAL1 region successfully\n",
            "üßπ Cleaning up temporary table...\n",
            "‚úÖ Temporary table deleted\n",
            "üîç Verifying imported data...\n",
            "üîç Checking table schema...\n",
            "‚úÖ Table found: graph-demo-471710:gdelt.gkg_partitioned\n",
            "   Rows: 381,975\n",
            "   Columns: 27\n",
            "   Partition-related columns: []\n",
            "üìä Imported data summary:\n",
            "   Total rows: 381,975\n",
            "‚ö†Ô∏è  _PARTITIONTIME column not found, skipping detailed verification\n",
            "üéâ Cross-region data copy completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Copy GDELT data for specific partition (September 11, 2025) - Cross-region approach\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime\n",
        "\n",
        "def copy_gdelt_partition_cross_region():\n",
        "    \"\"\"\n",
        "    Copy data from GDELT table (US region) to local table (US-CENTRAL1 region).\n",
        "    Uses a temporary table approach to handle cross-region data access.\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Starting cross-region GDELT data copy...\")\n",
        "    print(f\" Target date: September 11, 2025\")\n",
        "    print(f\" Source: {GDELT_TABLE} (US region)\")\n",
        "    print(f\" Destination: {GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE} (US-CENTRAL1 region)\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        local_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        # Step 0: Check if destination table already exists\n",
        "        print(\"üîç Checking if destination table already exists...\")\n",
        "        dest_table_ref = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}\"\n",
        "        try:\n",
        "            existing_dest_table = local_client.get_table(dest_table_ref)\n",
        "            print(f\"‚úÖ Destination table already exists: {existing_dest_table.full_table_id}\")\n",
        "            print(f\"   Rows: {existing_dest_table.num_rows:,}\")\n",
        "            print(\"‚è≠Ô∏è  Skipping data copy process, destination table already has data\")\n",
        "            \n",
        "            # Optional: Verify the data is for the correct date\n",
        "            print(\"üîç Verifying existing data...\")\n",
        "            try:\n",
        "                # Try a simple count first\n",
        "                simple_query = f\"SELECT COUNT(*) as row_count FROM `{dest_table_ref}`\"\n",
        "                result = local_client.query(simple_query, location=\"US-CENTRAL1\").result()\n",
        "                for row in result:\n",
        "                    print(f\"üìä Existing data summary:\")\n",
        "                    print(f\"   Total rows: {row.row_count:,}\")\n",
        "                    print(\"‚úÖ Data verification completed\")\n",
        "            except Exception as verify_error:\n",
        "                print(f\"‚ö†Ô∏è  Could not verify existing data: {verify_error}\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                print(\"üìù Destination table doesn't exist, proceeding with data copy...\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Error checking destination table: {e}\")\n",
        "                print(\"üìù Proceeding with data copy...\")\n",
        "        \n",
        "        # Step 1: Create dataset if it doesn't exist\n",
        "        print(f\" Checking if dataset '{BIGQUERY_DATASET}' exists...\")\n",
        "        dataset_ref = local_client.dataset(BIGQUERY_DATASET)\n",
        "        \n",
        "        try:\n",
        "            dataset = local_client.get_dataset(dataset_ref)\n",
        "            print(f\"‚úÖ Dataset '{BIGQUERY_DATASET}' already exists\")\n",
        "        except Exception:\n",
        "            print(f\"üìù Dataset '{BIGQUERY_DATASET}' doesn't exist, creating it...\")\n",
        "            \n",
        "            # Create dataset with proper location\n",
        "            dataset = bigquery.Dataset(dataset_ref)\n",
        "            dataset.location = \"US-CENTRAL1\"  # Specify the region\n",
        "            dataset.description = \"GDELT data for graph analysis\"\n",
        "            \n",
        "            dataset = local_client.create_dataset(dataset, timeout=30)\n",
        "            print(f\"‚úÖ Dataset '{BIGQUERY_DATASET}' created successfully in us-central1\")\n",
        "        \n",
        "        # Step 2: Create dataset in US region for temporary table\n",
        "        print(\"üìù Creating dataset in US region for temporary table...\")\n",
        "        us_dataset_name = f\"{BIGQUERY_DATASET}_us\"\n",
        "        us_dataset_ref = bigquery.DatasetReference(GCP_PROJECT_ID, us_dataset_name)\n",
        "        \n",
        "        try:\n",
        "            us_dataset = local_client.get_dataset(us_dataset_ref)\n",
        "            print(f\"‚úÖ Dataset '{us_dataset_name}' already exists in US region\")\n",
        "        except Exception as e:\n",
        "            if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                print(f\"üìù Creating dataset '{us_dataset_name}' in US region...\")\n",
        "                us_dataset = bigquery.Dataset(us_dataset_ref)\n",
        "                us_dataset.location = \"US\"\n",
        "                us_dataset.description = \"GDELT data for graph analysis (US region - temporary)\"\n",
        "                try:\n",
        "                    us_dataset = local_client.create_dataset(us_dataset, timeout=30)\n",
        "                    print(f\"‚úÖ Dataset '{us_dataset_name}' created in US region\")\n",
        "                except Exception as create_error:\n",
        "                    if \"Already Exists\" in str(create_error) or \"409\" in str(create_error):\n",
        "                        print(f\"‚úÖ Dataset '{us_dataset_name}' already exists in US region (created by another process)\")\n",
        "                    else:\n",
        "                        raise create_error\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Unexpected error checking dataset in US region: {e}\")\n",
        "                raise e\n",
        "        \n",
        "        # Step 3: Check if temporary table already exists, if not query GDELT data\n",
        "        temp_table_ref = local_client.dataset(us_dataset_name).table(f\"temp_{BIGQUERY_TABLE}\")\n",
        "        \n",
        "        print(\"üîç Checking if temporary table already exists...\")\n",
        "        try:\n",
        "            existing_temp_table = local_client.get_table(temp_table_ref)\n",
        "            print(f\"‚úÖ Temporary table already exists: {existing_temp_table.full_table_id}\")\n",
        "            print(f\"   Rows: {existing_temp_table.num_rows:,}\")\n",
        "            print(\"‚è≠Ô∏è  Skipping data query, using existing temporary table\")\n",
        "        except Exception as e:\n",
        "            if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                print(\"üìä Temporary table doesn't exist, querying GDELT data and saving to temporary table...\")\n",
        "                \n",
        "                # Configure the query job to save to temporary table in US region\n",
        "                job_config = bigquery.QueryJobConfig()\n",
        "                job_config.destination = temp_table_ref\n",
        "                job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "                job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
        "                \n",
        "                # Query the GDELT table\n",
        "                query = f\"\"\"\n",
        "                SELECT *\n",
        "                FROM `{GDELT_TABLE}`\n",
        "                WHERE _PARTITIONTIME = TIMESTAMP('2025-09-11')\n",
        "                \"\"\"\n",
        "                \n",
        "                print(\"üìä Executing query...\")\n",
        "                print(f\"üîç Query: {query}\")\n",
        "                print(f\"üéØ Destination: {GCP_PROJECT_ID}.{us_dataset_name}.temp_{BIGQUERY_TABLE}\")\n",
        "                \n",
        "                # Run the query - this will automatically handle cross-region data transfer\n",
        "                query_job = local_client.query(\n",
        "                    query,\n",
        "                    job_config=job_config,\n",
        "                    location=\"US\"  # Query in US region where GDELT table exists\n",
        "                )\n",
        "                \n",
        "                print(f\"‚è≥ Query job started: {query_job.job_id}\")\n",
        "                print(\"‚è≥ Waiting for query to complete...\")\n",
        "                query_job.result()  # Wait for job to complete\n",
        "                print(\"‚úÖ Data copied to temporary table in US region\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Unexpected error checking temporary table: {e}\")\n",
        "                raise e\n",
        "        \n",
        "        # Define source table reference\n",
        "        source_table_ref = bigquery.TableReference.from_string(f\"{GCP_PROJECT_ID}.{us_dataset_name}.temp_{BIGQUERY_TABLE}\")\n",
        "        \n",
        "        # Step 4: Copy data from US region temp table to US-CENTRAL1 region\n",
        "        print(\"üîÑ Copying data from US region to US-CENTRAL1 region...\")\n",
        "        \n",
        "        # Configure the copy job\n",
        "        copy_job_config = bigquery.CopyJobConfig()\n",
        "        copy_job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "        copy_job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
        "        \n",
        "        # Destination table (in US-CENTRAL1 region)\n",
        "        dest_table_ref = local_client.dataset(BIGQUERY_DATASET).table(BIGQUERY_TABLE)\n",
        "        \n",
        "        # Copy the data - need to specify source location\n",
        "        copy_job = local_client.copy_table(\n",
        "            source_table_ref,\n",
        "            dest_table_ref,\n",
        "            job_config=copy_job_config,\n",
        "            location=\"US\"  # Source is in US region\n",
        "        )\n",
        "        \n",
        "        print(f\"‚è≥ Copy job started: {copy_job.job_id}\")\n",
        "        print(\"‚è≥ Waiting for copy to complete...\")\n",
        "        copy_job.result()  # Wait for job to complete\n",
        "        print(\"‚úÖ Data copied to US-CENTRAL1 region successfully\")\n",
        "        \n",
        "        # Step 5: Clean up temporary table\n",
        "        print(\"üßπ Cleaning up temporary table...\")\n",
        "        try:\n",
        "            local_client.delete_table(source_table_ref)\n",
        "            print(\"‚úÖ Temporary table deleted\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not delete temporary table: {e}\")\n",
        "        \n",
        "        # Step 6: Verify the data\n",
        "        print(\"üîç Verifying imported data...\")\n",
        "        \n",
        "        # First, check what columns are available in the table\n",
        "        print(\"üîç Checking table schema...\")\n",
        "        try:\n",
        "            table = local_client.get_table(f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}\")\n",
        "            print(f\"‚úÖ Table found: {table.full_table_id}\")\n",
        "            print(f\"   Rows: {table.num_rows:,}\")\n",
        "            print(f\"   Columns: {len(table.schema)}\")\n",
        "            \n",
        "            # Check if _PARTITIONTIME column exists\n",
        "            partition_columns = [field.name for field in table.schema if 'partition' in field.name.lower() or 'time' in field.name.lower()]\n",
        "            print(f\"   Partition-related columns: {partition_columns}\")\n",
        "            \n",
        "            # Try a simple count query first\n",
        "            simple_verification_query = f\"\"\"\n",
        "            SELECT COUNT(*) as row_count\n",
        "            FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`\n",
        "            \"\"\"\n",
        "            \n",
        "            verification_result = local_client.query(simple_verification_query, location=\"US-CENTRAL1\").result()\n",
        "            for row in verification_result:\n",
        "                print(f\"üìä Imported data summary:\")\n",
        "                print(f\"   Total rows: {row.row_count:,}\")\n",
        "                \n",
        "            # If _PARTITIONTIME exists, try the full verification\n",
        "            if '_PARTITIONTIME' in [field.name for field in table.schema]:\n",
        "                print(\"üîç Running detailed verification with _PARTITIONTIME...\")\n",
        "                detailed_verification_query = f\"\"\"\n",
        "                SELECT \n",
        "                    COUNT(*) as row_count,\n",
        "                    MIN(_PARTITIONTIME) as min_partition_time,\n",
        "                    MAX(_PARTITIONTIME) as max_partition_time\n",
        "                FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`\n",
        "                WHERE _PARTITIONTIME = TIMESTAMP('2025-09-11')\n",
        "                \"\"\"\n",
        "                \n",
        "                detailed_result = local_client.query(detailed_verification_query, location=\"US-CENTRAL1\").result()\n",
        "                for row in detailed_result:\n",
        "                    print(f\"   Rows for 2025-09-11: {row.row_count:,}\")\n",
        "                    print(f\"   Min partition time: {row.min_partition_time}\")\n",
        "                    print(f\"   Max partition time: {row.max_partition_time}\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  _PARTITIONTIME column not found, skipping detailed verification\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during verification: {e}\")\n",
        "            print(\"üí° Table may have been created but verification failed\")\n",
        "        \n",
        "        print(\"üéâ Cross-region data copy completed successfully!\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during data copy: {e}\")\n",
        "        print(\"üí° Troubleshooting tips:\")\n",
        "        print(\"   - Check if the GDELT_TABLE exists and is accessible\")\n",
        "        print(\"   - Verify your project has BigQuery API enabled\")\n",
        "        print(\"   - Ensure you have the necessary permissions\")\n",
        "        return False\n",
        "\n",
        "# Run the cross-region copy process\n",
        "copy_success = copy_gdelt_partition_cross_region()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
