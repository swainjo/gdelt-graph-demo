{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GDELT Demo Data Prep\n",
        "\n",
        "This notebook demonstrates working with GDELT (Global Database of Events, Language, and Tone) data for graph analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuration Setup\n",
        "\n",
        "This cell defines all the essential configuration variables for the GDELT data analysis project:\n",
        "\n",
        "### **Project Settings**\n",
        "- **GCP_PROJECT_ID**: Your Google Cloud Platform project identifier\n",
        "- **PROJECT_REGION**: Target region for BigQuery operations (us-central1)\n",
        "- **BIGQUERY_DATASET**: Dataset name where GDELT data will be stored locally\n",
        "\n",
        "### **GDELT Source Settings**  \n",
        "- **GDELT_PROJECT_ID**: Public GDELT BigQuery project (gdelt-bq)\n",
        "- **GDELT_DATASET**: Public GDELT dataset (gdeltv2)\n",
        "- **GDELT_REGION**: Source region for GDELT data (US)\n",
        "\n",
        "### **Data Tables**\n",
        "- **BIGQUERY_TABLES**: List of GDELT tables to copy for analysis\n",
        "  - `gkg_partitioned`: Global Knowledge Graph data\n",
        "  - `events_partitioned`: Event data\n",
        "  - `eventmentions_partitioned`: Event mentions data\n",
        "\n",
        "### **Storage**\n",
        "- **GCS_BUCKET**: Google Cloud Storage bucket for data exports\n",
        "\n",
        "‚ö†Ô∏è **Important**: Update GCP_PROJECT_ID with your actual project ID before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  GCP Project: graph-demo-471710\n",
            "  BigQuery Dataset: gdelt\n",
            "  BigQuery Tables: ['gkg_partitioned', 'events_partitioned', 'eventmentions_partitioned']\n",
            "  GDELT Project: gdelt-bq\n",
            "  GDELT Dataset: gdeltv2\n",
            "  GDELT Region: us\n",
            "  GCS Bucket: gdelt_graph\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables\n",
        "GCP_PROJECT_ID = \"graph-demo-471710\"  # Replace with your actual GCP project ID\n",
        "PROJECT_REGION = \"us-central1\"\n",
        "BIGQUERY_DATASET = \"gdelt\"  # Replace with your actual BigQuery dataset name\n",
        "BIGQUERY_TABLES = [\"gkg_partitioned\", \"events_partitioned\",\"eventmentions_partitioned\"]  # List of tables to copy\n",
        "GDELT_PROJECT_ID = \"gdelt-bq\"\n",
        "GDELT_DATASET = \"gdeltv2\"  \n",
        "GDELT_REGION = \"us\"\n",
        "GCS_BUCKET = \"gdelt_graph\"\n",
        "\n",
        "# Derived variables - will be generated for each table\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  GCP Project: {GCP_PROJECT_ID}\")\n",
        "print(f\"  BigQuery Dataset: {BIGQUERY_DATASET}\")\n",
        "print(f\"  BigQuery Tables: {BIGQUERY_TABLES}\")\n",
        "print(f\"  GDELT Project: {GDELT_PROJECT_ID}\")\n",
        "print(f\"  GDELT Dataset: {GDELT_DATASET}\")\n",
        "print(f\"  GDELT Region: {GDELT_REGION}\")\n",
        "print(f\"  GCS Bucket: {GCS_BUCKET}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Library Imports\n",
        "\n",
        "This cell imports all necessary Python libraries for the GDELT data analysis workflow:\n",
        "\n",
        "### **Google Cloud Services**\n",
        "- `google.cloud.bigquery`: For querying and managing BigQuery data\n",
        "- `google.cloud.storage`: For Google Cloud Storage operations\n",
        "- `google.auth`: For GCP authentication handling\n",
        "\n",
        "### **Data Processing**\n",
        "- `pandas`: For data manipulation and analysis\n",
        "- `json`: For JSON data handling\n",
        "- `datetime`: For date/time operations\n",
        "\n",
        "### **Network Analysis & Visualization**\n",
        "- `networkx`: For creating and analyzing graph networks\n",
        "- `matplotlib.pyplot`: For creating visualizations and plots\n",
        "\n",
        "### **System & File Operations**\n",
        "- `os`, `pathlib.Path`: For file system operations\n",
        "- `subprocess`: For running system commands\n",
        "- `shutil`: For file operations\n",
        "\n",
        "All libraries are tested for successful import with confirmation messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n",
            "   - BigQuery and Cloud Storage clients ready\n",
            "   - NetworkX and Matplotlib ready for visualization\n",
            "   - Pandas ready for data processing\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import json\n",
        "from datetime import datetime\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import os\n",
        "import shutil\n",
        "from google.auth import default\n",
        "from google.auth.exceptions import DefaultCredentialsError\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"   - BigQuery and Cloud Storage clients ready\")\n",
        "print(\"   - NetworkX and Matplotlib ready for visualization\")\n",
        "print(\"   - Pandas ready for data processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê GCP Authentication Setup\n",
        "\n",
        "This cell provides a comprehensive GCP authentication function that handles various authentication scenarios:\n",
        "\n",
        "### **Authentication Process**\n",
        "1. **Credential Check**: Verifies existing Google Cloud credentials\n",
        "2. **Project Validation**: Ensures credentials match the target project\n",
        "3. **Credential Reset**: Clears old credentials if project mismatch detected\n",
        "4. **Project Configuration**: Sets the correct GCP project using gcloud CLI\n",
        "5. **Re-authentication**: Initiates browser-based OAuth flow if needed\n",
        "6. **Quota Project**: Sets quota project to avoid billing warnings\n",
        "7. **Verification**: Confirms successful authentication\n",
        "\n",
        "### **Error Handling**\n",
        "- Handles missing credentials gracefully\n",
        "- Provides manual fallback instructions\n",
        "- Manages project mismatches automatically\n",
        "- Shows detailed error messages and troubleshooting tips\n",
        "\n",
        "### **Environment Setup**\n",
        "- Sets `GOOGLE_CLOUD_PROJECT` environment variable\n",
        "- Configures application default credentials\n",
        "- Prepares credentials for BigQuery and Cloud Storage clients\n",
        "\n",
        "‚ö° **Note**: This function may open a browser window for OAuth authentication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê Setting up GCP Authentication...\n",
            "üîç Checking for existing credentials...\n",
            "‚úÖ Found existing credentials for project: graph-demo-471710\n",
            "üéØ Project matches target project: graph-demo-471710\n"
          ]
        }
      ],
      "source": [
        "# GCP Authentication Setup\n",
        "\n",
        "\n",
        "def setup_gcp_authentication():\n",
        "    \"\"\"Complete GCP authentication setup with error handling\"\"\"\n",
        "    print(\"üîê Setting up GCP Authentication...\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Try to use existing credentials first\n",
        "        print(\"üîç Checking for existing credentials...\")\n",
        "        try:\n",
        "            credentials, default_project = default()\n",
        "            print(f\"‚úÖ Found existing credentials for project: {default_project}\")\n",
        "            \n",
        "            # If the project matches, we're good\n",
        "            if default_project == GCP_PROJECT_ID:\n",
        "                print(f\"üéØ Project matches target project: {GCP_PROJECT_ID}\")\n",
        "                os.environ['GOOGLE_CLOUD_PROJECT'] = GCP_PROJECT_ID\n",
        "                return credentials, GCP_PROJECT_ID\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Project mismatch: {default_project} vs {GCP_PROJECT_ID}\")\n",
        "                print(\"üîÑ Will re-authenticate with correct project...\")\n",
        "        except DefaultCredentialsError:\n",
        "            print(\"‚ùå No existing credentials found\")\n",
        "            print(\"üîÑ Will authenticate from scratch...\")\n",
        "        \n",
        "        # Step 2: Clear old credentials if needed\n",
        "        print(\"üóëÔ∏è  Clearing old credentials...\")\n",
        "        adc_path = os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\")\n",
        "        if os.path.exists(adc_path):\n",
        "            os.remove(adc_path)\n",
        "            print(\"‚úÖ Removed old application default credentials\")\n",
        "        \n",
        "        # Step 3: Set the correct project\n",
        "        print(f\"üéØ Setting gcloud project to: {GCP_PROJECT_ID}\")\n",
        "        result = subprocess.run(['gcloud', 'config', 'set', 'project', GCP_PROJECT_ID], \n",
        "                              capture_output=True, text=True, check=True)\n",
        "        print(\"‚úÖ Project set successfully\")\n",
        "        \n",
        "        # Step 4: Re-authenticate\n",
        "        print(\"üîÑ Re-authenticating with application default credentials...\")\n",
        "        print(\"   This will open a browser window for authentication...\")\n",
        "        \n",
        "        result = subprocess.run(['gcloud', 'auth', 'application-default', 'login'], \n",
        "                              check=True)\n",
        "        print(\"‚úÖ Re-authentication successful\")\n",
        "        \n",
        "        # Step 5: Set quota project to avoid warnings\n",
        "        print(\"üí∞ Setting quota project...\")\n",
        "        try:\n",
        "            subprocess.run(['gcloud', 'auth', 'application-default', 'set-quota-project', GCP_PROJECT_ID], \n",
        "                          capture_output=True, text=True, check=True)\n",
        "            print(\"‚úÖ Quota project set successfully\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è  Could not set quota project (this is usually fine)\")\n",
        "        \n",
        "        # Step 6: Verify the setup\n",
        "        print(\"üß™ Verifying authentication...\")\n",
        "        credentials, project = default()\n",
        "        print(f\"‚úÖ Authentication successful - Project: {project}\")\n",
        "        \n",
        "        # Set environment variable\n",
        "        os.environ['GOOGLE_CLOUD_PROJECT'] = GCP_PROJECT_ID\n",
        "        print(f\"üåç Set GOOGLE_CLOUD_PROJECT environment variable to: {GCP_PROJECT_ID}\")\n",
        "        \n",
        "        return credentials, GCP_PROJECT_ID\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Command failed: {e}\")\n",
        "        print(\"üí° Manual steps required:\")\n",
        "        print(f\"   1. gcloud config set project {GCP_PROJECT_ID}\")\n",
        "        print(\"   2. gcloud auth application-default login\")\n",
        "        print(f\"   3. gcloud auth application-default set-quota-project {GCP_PROJECT_ID}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Run authentication setup\n",
        "credentials, authenticated_project = setup_gcp_authentication()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing GCP connectivity...\n",
            "‚úÖ Using authenticated project: graph-demo-471710\n",
            "üîó BigQuery client created for project: graph-demo-471710\n",
            "‚úÖ BigQuery connectivity successful - Test query result: 1\n",
            "‚úÖ BigQuery dataset 'gdelt' accessible\n",
            "üìä Found 12 tables in dataset\n",
            "   - article\n",
            "   - event\n",
            "   - event_participant\n",
            "   - eventmentions_partitioned\n",
            "   - events_partitioned\n",
            "   ... and 7 more tables\n",
            "‚úÖ Cloud Storage connectivity successful - Found 1 buckets\n",
            "üéâ All GCP connectivity tests passed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test GCP connectivity\n",
        "def test_gcp_connectivity():\n",
        "    \"\"\"Test basic connectivity to GCP services\"\"\"\n",
        "    print(\"üîç Testing GCP connectivity...\")\n",
        "    \n",
        "    # Check if authentication was successful\n",
        "    if not credentials or not authenticated_project:\n",
        "        print(\"‚ùå Authentication required - please run the authentication cell first\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ Using authenticated project: {authenticated_project}\")\n",
        "    \n",
        "    # Test 1: Test BigQuery connectivity\n",
        "    try:\n",
        "        # Use explicit credentials and project\n",
        "        client = bigquery.Client(credentials=credentials, project=authenticated_project)\n",
        "        print(f\"üîó BigQuery client created for project: {client.project}\")\n",
        "        \n",
        "        # Simple query to test connectivity\n",
        "        query = \"SELECT 1 as test_value\"\n",
        "        result = client.query(query).result()\n",
        "        for row in result:\n",
        "            print(f\"‚úÖ BigQuery connectivity successful - Test query result: {row.test_value}\")\n",
        "            break  # Only need first row\n",
        "    except Exception as e:\n",
        "        error_str = str(e)\n",
        "        if \"has been deleted\" in error_str or \"USER_PROJECT_DENIED\" in error_str:\n",
        "            print(f\"‚ùå BigQuery connectivity failed: Project mismatch detected\")\n",
        "            print(f\"   Error: {e}\")\n",
        "            print(f\"üîß This usually means your credentials are cached for a different project\")\n",
        "            print(f\"   üí° Try running the authentication cell again\")\n",
        "            print(f\"   üìã Or manually run: gcloud auth application-default login\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"‚ùå BigQuery connectivity failed: {e}\")\n",
        "            return False\n",
        "    \n",
        "    # Test 2: Test BigQuery dataset access\n",
        "    try:\n",
        "        client = bigquery.Client(credentials=credentials, project=authenticated_project)\n",
        "        dataset_ref = client.dataset(BIGQUERY_DATASET)\n",
        "        dataset = client.get_dataset(dataset_ref)\n",
        "        print(f\"‚úÖ BigQuery dataset '{BIGQUERY_DATASET}' accessible\")\n",
        "        \n",
        "        # List tables in the dataset\n",
        "        tables = list(client.list_tables(dataset_ref))\n",
        "        print(f\"üìä Found {len(tables)} tables in dataset\")\n",
        "        for table in tables[:5]:  # Show first 5 tables\n",
        "            print(f\"   - {table.table_id}\")\n",
        "        if len(tables) > 5:\n",
        "            print(f\"   ... and {len(tables) - 5} more tables\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå BigQuery dataset access failed: {e}\")\n",
        "        print(f\"   Make sure dataset '{BIGQUERY_DATASET}' exists in project '{authenticated_project}'\")\n",
        "        return False\n",
        "    \n",
        "    # Test 3: Test Cloud Storage connectivity\n",
        "    try:\n",
        "        storage_client = storage.Client(credentials=credentials, project=authenticated_project)\n",
        "        # List buckets to test connectivity\n",
        "        buckets = list(storage_client.list_buckets())\n",
        "        print(f\"‚úÖ Cloud Storage connectivity successful - Found {len(buckets)} buckets\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cloud Storage connectivity failed: {e}\")\n",
        "        return False\n",
        "    \n",
        "    print(\"üéâ All GCP connectivity tests passed!\")\n",
        "    return True\n",
        "\n",
        "# Run the connectivity test\n",
        "test_gcp_connectivity()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéâ Setup complete! Ready to work with GDELT data.\n",
            "üìä Project: graph-demo-471710\n",
            "üóÑÔ∏è  Dataset: gdelt\n",
            "üöÄ You can now run queries against your GDELT data!\n"
          ]
        }
      ],
      "source": [
        "# Ready for GDELT analysis!\n",
        "print(\"üéâ Setup complete! Ready to work with GDELT data.\")\n",
        "print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "print(\"üöÄ You can now run queries against your GDELT data!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä GDELT Dataset Discovery\n",
        "\n",
        "This cell explores the public GDELT BigQuery project to understand available datasets and tables:\n",
        "\n",
        "### **Dataset Exploration**\n",
        "- Connects to the public GDELT project (`gdelt-bq`)\n",
        "- Lists all available datasets in the GDELT project\n",
        "- Provides metadata for each dataset (creation date, location, description)\n",
        "- Counts tables within each dataset\n",
        "\n",
        "### **Information Gathered**\n",
        "- **Dataset Names**: All available GDELT datasets\n",
        "- **Table Counts**: Number of tables in each dataset\n",
        "- **Creation/Modification Dates**: When datasets were last updated\n",
        "- **Geographic Location**: Where datasets are stored (typically US region)\n",
        "- **Sample Tables**: Preview of table names in each dataset\n",
        "\n",
        "### **Purpose**\n",
        "- Helps understand the structure of public GDELT data\n",
        "- Identifies which datasets contain the tables we need\n",
        "- Provides context for the data import process\n",
        "- Assists in troubleshooting data access issues\n",
        "\n",
        "üìã **Output**: Detailed listing of all GDELT datasets with metadata and table information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Listing datasets in GDELT project: gdelt-bq\n",
            "‚úÖ Connected to GDELT project: gdelt-bq\n",
            "üìä Found 8 datasets in gdelt-bq:\n",
            "------------------------------------------------------------\n",
            "üìÅ Dataset: covid19\n",
            "   Description: No description\n",
            "   Created: 2020-03-30 00:36:39.040000+00:00\n",
            "   Modified: 2024-11-19 22:08:34.013000+00:00\n",
            "   Location: US\n",
            "   Tables: 3\n",
            "   Sample tables:\n",
            "     - onlinenews\n",
            "     - onlinenewsgeo\n",
            "     - tvnews\n",
            "\n",
            "üìÅ Dataset: extra\n",
            "   Description: No description\n",
            "   Created: 2014-12-03 05:22:28.037000+00:00\n",
            "   Modified: 2024-11-19 22:09:01.418000+00:00\n",
            "   Location: US\n",
            "   Tables: 6\n",
            "   Sample tables:\n",
            "     - countries_by_media_50pct\n",
            "     - countrygeolookup\n",
            "     - countryinfo\n",
            "     - countryinfo2\n",
            "     - sourcesbycountry\n",
            "     ... and 1 more\n",
            "\n",
            "üìÅ Dataset: full\n",
            "   Description: No description\n",
            "   Created: 2014-04-23 23:04:48.708000+00:00\n",
            "   Modified: 2024-11-19 22:05:19.124000+00:00\n",
            "   Location: US\n",
            "   Tables: 3\n",
            "   Sample tables:\n",
            "     - crosswalk_geocountrycodetohuman\n",
            "     - events\n",
            "     - events_partitioned\n",
            "\n",
            "üìÅ Dataset: gdeltv2\n",
            "   Description: No description\n",
            "   Created: 2015-02-18 20:37:17.892000+00:00\n",
            "   Modified: 2024-11-19 22:08:07.818000+00:00\n",
            "   Location: US\n",
            "   Tables: 62\n",
            "   Sample tables:\n",
            "     - academicliteraturegkg\n",
            "     - cloudvision\n",
            "     - cloudvision_partitioned\n",
            "     - cloudvision_webentitygraph_20191020\n",
            "     - domainsbycountry_alllangs_april2015\n",
            "     ... and 57 more\n",
            "\n",
            "üìÅ Dataset: gdeltv2_ngrams\n",
            "   Description: No description\n",
            "   Created: 2016-06-09 23:38:25.995000+00:00\n",
            "   Modified: 2024-11-19 22:07:01.809000+00:00\n",
            "   Location: US\n",
            "   Tables: 1\n",
            "   Sample tables:\n",
            "     - arabic_trigram\n",
            "\n",
            "üìÅ Dataset: hathitrustbooks\n",
            "   Description: No description\n",
            "   Created: 2015-09-08 14:26:12.973000+00:00\n",
            "   Modified: 2024-11-19 22:07:40.211000+00:00\n",
            "   Location: US\n",
            "   Tables: 213\n",
            "   Sample tables:\n",
            "     - 1800\n",
            "     - 1801\n",
            "     - 1802\n",
            "     - 1803\n",
            "     - 1804\n",
            "     ... and 208 more\n",
            "\n",
            "üìÅ Dataset: internetarchivebooks\n",
            "   Description: No description\n",
            "   Created: 2015-08-27 01:25:47.069000+00:00\n",
            "   Modified: 2024-11-19 22:06:35.918000+00:00\n",
            "   Location: US\n",
            "   Tables: 215\n",
            "   Sample tables:\n",
            "     - 1800\n",
            "     - 1801\n",
            "     - 1802\n",
            "     - 1803\n",
            "     - 1804\n",
            "     ... and 210 more\n",
            "\n",
            "üìÅ Dataset: sample_views\n",
            "   Description: No description\n",
            "   Created: 2014-04-24 04:50:03.517000+00:00\n",
            "   Modified: 2024-11-19 22:05:56.624000+00:00\n",
            "   Location: US\n",
            "   Tables: 10\n",
            "   Sample tables:\n",
            "     - actor_combinations_by_year\n",
            "     - actor_combinations_top1_by_year\n",
            "     - country_date_count\n",
            "     - country_date_matconf_numarts\n",
            "     - country_date_matconfbyactiongeo\n",
            "     ... and 5 more\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List datasets in the GDELT_PROJECT_ID project\n",
        "def list_gdelt_datasets():\n",
        "    \"\"\"List all datasets in the GDELT_PROJECT_ID project\"\"\"\n",
        "    print(f\"üîç Listing datasets in GDELT project: {GDELT_PROJECT_ID}\")\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client for the GDELT project\n",
        "        gdelt_client = bigquery.Client(project=GDELT_PROJECT_ID)\n",
        "        print(f\"‚úÖ Connected to GDELT project: {gdelt_client.project}\")\n",
        "        \n",
        "        # List all datasets in the project\n",
        "        datasets = list(gdelt_client.list_datasets())\n",
        "        \n",
        "        if not datasets:\n",
        "            print(\"üì≠ No datasets found in the GDELT project\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"üìä Found {len(datasets)} datasets in {GDELT_PROJECT_ID}:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        dataset_info = []\n",
        "        for dataset in datasets:\n",
        "            # Get dataset details\n",
        "            dataset_ref = gdelt_client.dataset(dataset.dataset_id)\n",
        "            full_dataset = gdelt_client.get_dataset(dataset_ref)\n",
        "            \n",
        "            # Count tables in the dataset\n",
        "            tables = list(gdelt_client.list_tables(dataset_ref))\n",
        "            \n",
        "            info = {\n",
        "                'dataset_id': dataset.dataset_id,\n",
        "                'description': full_dataset.description or 'No description',\n",
        "                'created': full_dataset.created,\n",
        "                'modified': full_dataset.modified,\n",
        "                'location': full_dataset.location,\n",
        "                'table_count': len(tables)\n",
        "            }\n",
        "            dataset_info.append(info)\n",
        "            \n",
        "            print(f\"üìÅ Dataset: {dataset.dataset_id}\")\n",
        "            print(f\"   Description: {info['description']}\")\n",
        "            print(f\"   Created: {info['created']}\")\n",
        "            print(f\"   Modified: {info['modified']}\")\n",
        "            print(f\"   Location: {info['location']}\")\n",
        "            print(f\"   Tables: {info['table_count']}\")\n",
        "            \n",
        "            # Show first few tables if any\n",
        "            if tables:\n",
        "                print(f\"   Sample tables:\")\n",
        "                for table in tables[:5]:\n",
        "                    print(f\"     - {table.table_id}\")\n",
        "                if len(tables) > 5:\n",
        "                    print(f\"     ... and {len(tables) - 5} more\")\n",
        "            print()\n",
        "        \n",
        "        return dataset_info\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error listing datasets: {e}\")\n",
        "        return []\n",
        "\n",
        "# Run the function to list datasets\n",
        "gdelt_datasets = list_gdelt_datasets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Region GDELT Data Copy Function\n",
        "\n",
        "This function efficiently copies GDELT data from the US region to your local US-CENTRAL1 region using a smart multi-step approach:\n",
        "\n",
        "### üéØ **Purpose**\n",
        "- Copies GDELT data for a specific date (September 11, 2025) from the public GDELT dataset\n",
        "- Handles cross-region data transfer from US region to US-CENTRAL1 region\n",
        "- Optimizes for cost and speed with intelligent caching\n",
        "\n",
        "### üîÑ **Process Flow**\n",
        "1. **Destination Check**: Verifies if target table already exists (skips if data present)\n",
        "2. **Dataset Setup**: Creates required datasets in both US and US-CENTRAL1 regions\n",
        "3. **Temporary Table Check**: Checks if temp table exists in US region (reuses if available)\n",
        "4. **Data Query**: Queries GDELT data and saves to temporary table in US region\n",
        "5. **Cross-Region Copy**: Copies data from US region to US-CENTRAL1 region\n",
        "6. **Cleanup**: Removes temporary table and verifies final data\n",
        "\n",
        "### ‚ö° **Optimizations**\n",
        "- **Smart Caching**: Skips expensive operations if data already exists\n",
        "- **Cost Efficient**: Reuses temporary tables when possible\n",
        "- **Error Resilient**: Handles various BigQuery errors gracefully\n",
        "- **Progress Tracking**: Detailed logging throughout the process\n",
        "\n",
        "### üìä **Output**\n",
        "- Creates table: `{GCP_PROJECT_ID}.gdelt.gkg_partitioned` in US-CENTRAL1 region\n",
        "- Shows row counts and verification details\n",
        "- Provides troubleshooting tips if errors occur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy GDELT data for specific partition (September 11, 2025) - Cross-region approach for multiple tables\n",
        "\n",
        "\n",
        "def copy_gdelt_partition_cross_region():\n",
        "    \"\"\"\n",
        "    Copy data from GDELT tables (US region) to local tables (US-CENTRAL1 region).\n",
        "    Uses a temporary table approach to handle cross-region data access.\n",
        "    Processes each table in BIGQUERY_TABLES list.\n",
        "    \"\"\"\n",
        "    print(\"ÔøΩÔøΩ Starting cross-region GDELT data copy for multiple tables...\")\n",
        "    print(f\" Target date: September 11, 2025\")\n",
        "    print(f\" Source: GDELT tables in {GDELT_PROJECT_ID}.{GDELT_DATASET} (US region)\")\n",
        "    print(f\" Destination: {GCP_PROJECT_ID}.{BIGQUERY_DATASET} (US-CENTRAL1 region)\")\n",
        "    print(f\" Tables to process: {BIGQUERY_TABLES}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        local_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        # Step 0: Create dataset if it doesn't exist (same for all tables)\n",
        "        print(f\" Checking if dataset '{BIGQUERY_DATASET}' exists...\")\n",
        "        dataset_ref = local_client.dataset(BIGQUERY_DATASET)\n",
        "        \n",
        "        try:\n",
        "            dataset = local_client.get_dataset(dataset_ref)\n",
        "            print(f\"‚úÖ Dataset '{BIGQUERY_DATASET}' already exists\")\n",
        "        except Exception:\n",
        "            print(f\"üìù Dataset '{BIGQUERY_DATASET}' doesn't exist, creating it...\")\n",
        "            \n",
        "            # Create dataset with proper location\n",
        "            dataset = bigquery.Dataset(dataset_ref)\n",
        "            dataset.location = \"US-CENTRAL1\"  # Specify the region\n",
        "            dataset.description = \"GDELT data for graph analysis\"\n",
        "            \n",
        "            dataset = local_client.create_dataset(dataset, timeout=30)\n",
        "            print(f\"‚úÖ Dataset '{BIGQUERY_DATASET}' created successfully in us-central1\")\n",
        "        \n",
        "        # Step 1: Create dataset in US region for temporary tables\n",
        "        print(\"üìù Creating dataset in US region for temporary tables...\")\n",
        "        us_dataset_name = f\"{BIGQUERY_DATASET}_us\"\n",
        "        us_dataset_ref = bigquery.DatasetReference(GCP_PROJECT_ID, us_dataset_name)\n",
        "        \n",
        "        try:\n",
        "            us_dataset = local_client.get_dataset(us_dataset_ref)\n",
        "            print(f\"‚úÖ Dataset '{us_dataset_name}' already exists in US region\")\n",
        "        except Exception as e:\n",
        "            if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                print(f\"üìù Creating dataset '{us_dataset_name}' in US region...\")\n",
        "                us_dataset = bigquery.Dataset(us_dataset_ref)\n",
        "                us_dataset.location = \"US\"\n",
        "                us_dataset.description = \"GDELT data for graph analysis (US region - temporary)\"\n",
        "                try:\n",
        "                    us_dataset = local_client.create_dataset(us_dataset, timeout=30)\n",
        "                    print(f\"‚úÖ Dataset '{us_dataset_name}' created in US region\")\n",
        "                except Exception as create_error:\n",
        "                    if \"Already Exists\" in str(create_error) or \"409\" in str(create_error):\n",
        "                        print(f\"‚úÖ Dataset '{us_dataset_name}' already exists in US region (created by another process)\")\n",
        "                    else:\n",
        "                        raise create_error\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Unexpected error checking dataset in US region: {e}\")\n",
        "                raise e\n",
        "        \n",
        "        # Process each table\n",
        "        for i, table_name in enumerate(BIGQUERY_TABLES, 1):\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"ÔøΩÔøΩ Processing table {i}/{len(BIGQUERY_TABLES)}: {table_name}\")\n",
        "            print(f\"{'='*80}\")\n",
        "            \n",
        "            try:\n",
        "                # Create GDELT table reference for this table\n",
        "                gdelt_table = f\"{GDELT_PROJECT_ID}.{GDELT_DATASET}.{table_name}\"\n",
        "                \n",
        "                # Check if destination table already exists\n",
        "                print(f\"üîç Checking if destination table '{table_name}' already exists...\")\n",
        "                dest_table_ref = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\"\n",
        "                try:\n",
        "                    existing_dest_table = local_client.get_table(dest_table_ref)\n",
        "                    print(f\"‚úÖ Destination table already exists: {existing_dest_table.full_table_id}\")\n",
        "                    print(f\"   Rows: {existing_dest_table.num_rows:,}\")\n",
        "                    print(\"‚è≠Ô∏è  Skipping data copy process, destination table already has data\")\n",
        "                    \n",
        "                    # Optional: Verify the data is for the correct date\n",
        "                    print(\"üîç Verifying existing data...\")\n",
        "                    try:\n",
        "                        simple_query = f\"SELECT COUNT(*) as row_count FROM `{dest_table_ref}`\"\n",
        "                        result = local_client.query(simple_query, location=\"US-CENTRAL1\").result()\n",
        "                        for row in result:\n",
        "                            print(f\"ÔøΩÔøΩ Existing data summary:\")\n",
        "                            print(f\"   Total rows: {row.row_count:,}\")\n",
        "                            print(\"‚úÖ Data verification completed\")\n",
        "                    except Exception as verify_error:\n",
        "                        print(f\"‚ö†Ô∏è  Could not verify existing data: {verify_error}\")\n",
        "                    \n",
        "                    results[table_name] = {\"status\": \"skipped\", \"reason\": \"already_exists\"}\n",
        "                    continue\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                        print(\"üìù Destination table doesn't exist, proceeding with data copy...\")\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è  Error checking destination table: {e}\")\n",
        "                        print(\"üìù Proceeding with data copy...\")\n",
        "                \n",
        "                # Check if temporary table already exists, if not query GDELT data\n",
        "                temp_table_ref = local_client.dataset(us_dataset_name).table(f\"temp_{table_name}\")\n",
        "                \n",
        "                print(\"üîç Checking if temporary table already exists...\")\n",
        "                try:\n",
        "                    existing_temp_table = local_client.get_table(temp_table_ref)\n",
        "                    print(f\"‚úÖ Temporary table already exists: {existing_temp_table.full_table_id}\")\n",
        "                    print(f\"   Rows: {existing_temp_table.num_rows:,}\")\n",
        "                    print(\"‚è≠Ô∏è  Skipping data query, using existing temporary table\")\n",
        "                except Exception as e:\n",
        "                    if \"notFound\" in str(e) or \"404\" in str(e):\n",
        "                        print(\"üìä Temporary table doesn't exist, querying GDELT data and saving to temporary table...\")\n",
        "                        \n",
        "                        # Configure the query job to save to temporary table in US region\n",
        "                        job_config = bigquery.QueryJobConfig()\n",
        "                        job_config.destination = temp_table_ref\n",
        "                        job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "                        job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
        "                        \n",
        "                        # Query the GDELT table\n",
        "                        query = f\"\"\"\n",
        "                        SELECT *\n",
        "                        FROM `{gdelt_table}`\n",
        "                        WHERE _PARTITIONTIME = TIMESTAMP('2025-09-11')\n",
        "                        \"\"\"\n",
        "                        \n",
        "                        print(\"üìä Executing query...\")\n",
        "                        print(f\"üîç Query: {query}\")\n",
        "                        print(f\"üéØ Destination: {GCP_PROJECT_ID}.{us_dataset_name}.temp_{table_name}\")\n",
        "                        \n",
        "                        # Run the query - this will automatically handle cross-region data transfer\n",
        "                        query_job = local_client.query(\n",
        "                            query,\n",
        "                            job_config=job_config,\n",
        "                            location=\"US\"  # Query in US region where GDELT table exists\n",
        "                        )\n",
        "                        \n",
        "                        print(f\"‚è≥ Query job started: {query_job.job_id}\")\n",
        "                        print(\"‚è≥ Waiting for query to complete...\")\n",
        "                        query_job.result()  # Wait for job to complete\n",
        "                        print(\"‚úÖ Data copied to temporary table in US region\")\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è  Unexpected error checking temporary table: {e}\")\n",
        "                        raise e\n",
        "                \n",
        "                # Define source table reference\n",
        "                source_table_ref = bigquery.TableReference.from_string(f\"{GCP_PROJECT_ID}.{us_dataset_name}.temp_{table_name}\")\n",
        "                \n",
        "                # Copy data from US region temp table to US-CENTRAL1 region\n",
        "                print(\"üîÑ Copying data from US region to US-CENTRAL1 region...\")\n",
        "                \n",
        "                # Configure the copy job\n",
        "                copy_job_config = bigquery.CopyJobConfig()\n",
        "                copy_job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "                copy_job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
        "                \n",
        "                # Destination table (in US-CENTRAL1 region)\n",
        "                dest_table_ref = local_client.dataset(BIGQUERY_DATASET).table(table_name)\n",
        "                \n",
        "                # Copy the data - need to specify source location\n",
        "                copy_job = local_client.copy_table(\n",
        "                    source_table_ref,\n",
        "                    dest_table_ref,\n",
        "                    job_config=copy_job_config,\n",
        "                    location=\"US\"  # Source is in US region\n",
        "                )\n",
        "                \n",
        "                print(f\"‚è≥ Copy job started: {copy_job.job_id}\")\n",
        "                print(\"‚è≥ Waiting for copy to complete...\")\n",
        "                copy_job.result()  # Wait for job to complete\n",
        "                print(\"‚úÖ Data copied to US-CENTRAL1 region successfully\")\n",
        "                \n",
        "                # Clean up temporary table\n",
        "                print(\"üßπ Cleaning up temporary table...\")\n",
        "                try:\n",
        "                    local_client.delete_table(source_table_ref)\n",
        "                    print(\"‚úÖ Temporary table deleted\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Could not delete temporary table: {e}\")\n",
        "                \n",
        "                # Verify the data\n",
        "                print(\"üîç Verifying imported data...\")\n",
        "                \n",
        "                try:\n",
        "                    table = local_client.get_table(f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\")\n",
        "                    print(f\"‚úÖ Table found: {table.full_table_id}\")\n",
        "                    print(f\"   Rows: {table.num_rows:,}\")\n",
        "                    print(f\"   Columns: {len(table.schema)}\")\n",
        "                    \n",
        "                    # Try a simple count query first\n",
        "                    simple_verification_query = f\"\"\"\n",
        "                    SELECT COUNT(*) as row_count\n",
        "                    FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}`\n",
        "                    \"\"\"\n",
        "                    \n",
        "                    verification_result = local_client.query(simple_verification_query, location=\"US-CENTRAL1\").result()\n",
        "                    for row in verification_result:\n",
        "                        print(f\"üìä Imported data summary:\")\n",
        "                        print(f\"   Total rows: {row.row_count:,}\")\n",
        "                        \n",
        "                    results[table_name] = {\"status\": \"success\", \"rows\": table.num_rows}\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error during verification: {e}\")\n",
        "                    print(\"üí° Table may have been created but verification failed\")\n",
        "                    results[table_name] = {\"status\": \"partial_success\", \"error\": str(e)}\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error processing table '{table_name}': {e}\")\n",
        "                results[table_name] = {\"status\": \"failed\", \"error\": str(e)}\n",
        "                print(f\"‚ö†Ô∏è  Continuing with next table...\")\n",
        "        \n",
        "        # Print final summary\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"üìä FINAL SUMMARY\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        success_count = sum(1 for r in results.values() if r[\"status\"] == \"success\")\n",
        "        skipped_count = sum(1 for r in results.values() if r[\"status\"] == \"skipped\")\n",
        "        failed_count = sum(1 for r in results.values() if r[\"status\"] == \"failed\")\n",
        "        partial_count = sum(1 for r in results.values() if r[\"status\"] == \"partial_success\")\n",
        "        \n",
        "        print(f\"‚úÖ Successfully processed: {success_count}/{len(BIGQUERY_TABLES)} tables\")\n",
        "        print(f\"‚è≠Ô∏è  Skipped (already exists): {skipped_count}/{len(BIGQUERY_TABLES)} tables\")\n",
        "        print(f\"‚ö†Ô∏è  Partial success: {partial_count}/{len(BIGQUERY_TABLES)} tables\")\n",
        "        print(f\"‚ùå Failed: {failed_count}/{len(BIGQUERY_TABLES)} tables\")\n",
        "        \n",
        "        print(f\"\\nDetailed results:\")\n",
        "        for table_name, result in results.items():\n",
        "            status_emoji = {\n",
        "                \"success\": \"‚úÖ\",\n",
        "                \"skipped\": \"‚è≠Ô∏è\",\n",
        "                \"partial_success\": \"‚ö†Ô∏è\",\n",
        "                \"failed\": \"‚ùå\"\n",
        "            }.get(result[\"status\"], \"‚ùì\")\n",
        "            \n",
        "            print(f\"  {status_emoji} {table_name}: {result['status']}\")\n",
        "            if \"rows\" in result:\n",
        "                print(f\"      Rows: {result['rows']:,}\")\n",
        "            if \"error\" in result:\n",
        "                print(f\"      Error: {result['error']}\")\n",
        "        \n",
        "        print(f\"\\nüéâ Multi-table cross-region data copy completed!\")\n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Critical error during data copy: {e}\")\n",
        "        print(\"üí° Troubleshooting tips:\")\n",
        "        print(\"   - Check if the GDELT tables exist and are accessible\")\n",
        "        print(\"   - Verify your project has BigQuery API enabled\")\n",
        "        print(\"   - Ensure you have the necessary permissions\")\n",
        "        return results\n",
        "\n",
        "# Run the cross-region copy process for multiple tables\n",
        "copy_results = copy_gdelt_partition_cross_region()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• Person Co-occurrence Analysis\n",
        "\n",
        "This cell performs advanced person co-occurrence analysis on GDELT data to identify relationships between individuals:\n",
        "\n",
        "### **Query Functionality**\n",
        "- **Person Extraction**: Parses person names from GDELT V2Persons field\n",
        "- **Name Cleaning**: Removes suffixes and standardizes person names\n",
        "- **Co-occurrence Detection**: Finds people mentioned together in the same articles\n",
        "- **Relationship Scoring**: Counts frequency of co-appearances\n",
        "- **Flexible Filtering**: Can search for specific person or analyze all relationships\n",
        "\n",
        "### **SQL Analysis Process**\n",
        "1. **Data Extraction**: Unnests person lists from V2Persons field\n",
        "2. **Name Standardization**: Cleans and normalizes person names\n",
        "3. **Self-Join**: Matches articles to find person pairs\n",
        "4. **Aggregation**: Counts co-occurrence frequencies\n",
        "5. **Ranking**: Orders results by relationship strength\n",
        "\n",
        "### **Output Format**\n",
        "- **Person Pairs**: Two-person combinations (name1, name2)\n",
        "- **Co-occurrence Count**: Number of articles mentioning both people\n",
        "- **Relationship Strength**: Frequency-based scoring\n",
        "- **Top Results**: Limited to 25,000 strongest relationships\n",
        "\n",
        "### **Use Cases**\n",
        "- Political network analysis\n",
        "- Media relationship mapping\n",
        "- Influence pattern detection\n",
        "- Social network construction\n",
        "\n",
        "üîç **Configurable**: Set `search_person` variable to focus on specific individual or leave empty for all relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query to find person co-occurrence patterns\n",
        "def query_person_cooccurrence(search_person=\"\"):\n",
        "    \"\"\"\n",
        "    Query to find person co-occurrence patterns for a specified person in GDELT data.\n",
        "    This query identifies which people appear together with the specified person in the same articles.\n",
        "    If search_person is empty, returns all person co-occurrence patterns.\n",
        "    \"\"\"\n",
        "    \n",
        "    if search_person:\n",
        "        print(f\"üîç Querying person co-occurrence patterns for '{search_person}'...\")\n",
        "        where_clause = f\"WHERE V2Persons LIKE '%{search_person}%'\"\n",
        "    else:\n",
        "        print(\"üîç Querying all person co-occurrence patterns...\")\n",
        "        where_clause = \"\"\n",
        "    \n",
        "    # The BigQuery SQL query\n",
        "    query = f\"\"\"\n",
        "    WITH ArticleNames AS (\n",
        "      SELECT DISTINCT  -- DISTINCT moves here to apply to the whole row\n",
        "        GKGRECORDID,\n",
        "        REGEXP_REPLACE(person, r',.*', '') AS name -- DISTINCT removed from this line\n",
        "      FROM\n",
        "        `gdelt.gkg_partitioned`,\n",
        "        UNNEST(SPLIT(V2Persons, ';')) AS person\n",
        "      {where_clause}\n",
        "    )\n",
        "    -- This section creates the pairs by joining the table to itself\n",
        "    SELECT\n",
        "      a.name AS name1,\n",
        "      b.name AS name2,\n",
        "      COUNT(*) AS pair_count\n",
        "    FROM\n",
        "      ArticleNames AS a\n",
        "    JOIN\n",
        "      ArticleNames AS b ON a.GKGRECORDID = b.GKGRECORDID\n",
        "    WHERE\n",
        "      a.name < b.name -- This avoids duplicates and self-pairs\n",
        "    GROUP BY\n",
        "      1, 2\n",
        "    ORDER BY\n",
        "      pair_count DESC\n",
        "    LIMIT 25000;  \n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(f\"‚úÖ Connected to BigQuery project: {GCP_PROJECT_ID}\")\n",
        "        \n",
        "        # Execute the query\n",
        "        print(\"üìä Executing query...\")\n",
        "        if search_person:\n",
        "            print(f\"üîç Query: Finding person co-occurrence patterns for '{search_person}'\")\n",
        "        else:\n",
        "            print(\"üîç Query: Finding all person co-occurrence patterns\")\n",
        "        \n",
        "        query_job = client.query(query, location=\"US-CENTRAL1\")\n",
        "        results = query_job.result()\n",
        "        \n",
        "        # Process results manually (simple approach)\n",
        "        print(\"üìã Processing results...\")\n",
        "        rows = []\n",
        "        for row in results:\n",
        "            rows.append({\n",
        "                'name1': row.name1,\n",
        "                'name2': row.name2,\n",
        "                'pair_count': row.pair_count\n",
        "            })\n",
        "        \n",
        "        # Create DataFrame manually\n",
        "        df = pd.DataFrame(rows)\n",
        "        print(\"‚úÖ Results processed successfully\")\n",
        "        \n",
        "        print(f\"‚úÖ Query completed successfully!\")\n",
        "        print(f\"üìä Found {len(df)} person co-occurrence pairs\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        if search_person:\n",
        "            print(f\"üìà TOP PERSON CO-OCCURRENCE PATTERNS WITH '{search_person.upper()}'\")\n",
        "        else:\n",
        "            print(\"üìà TOP PERSON CO-OCCURRENCE PATTERNS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        if len(df) > 0:\n",
        "            # Display the results\n",
        "            print(df.to_string(index=False))\n",
        "            \n",
        "            # Show some statistics\n",
        "            print(f\"\\nüìä Summary Statistics:\")\n",
        "            print(f\"   Total pairs found: {len(df)}\")\n",
        "            print(f\"   Highest co-occurrence count: {df['pair_count'].max()}\")\n",
        "            print(f\"   Average co-occurrence count: {df['pair_count'].mean():.2f}\")\n",
        "            \n",
        "            # Show the top 10 most frequent co-occurrences\n",
        "            print(f\"\\nüèÜ TOP 10 MOST FREQUENT CO-OCCURRENCES:\")\n",
        "            print(\"-\" * 60)\n",
        "            top_10 = df.head(10)\n",
        "            for idx, row in top_10.iterrows():\n",
        "                print(f\"{idx+1:2d}. {row['name1']} & {row['name2']} - {row['pair_count']} times\")\n",
        "        else:\n",
        "            print(f\"‚ùå No co-occurrence patterns found for '{search_person}'\")\n",
        "            print(\"üí° This could mean:\")\n",
        "            print(f\"   - No articles contain '{search_person}' in the V2Persons field\")\n",
        "            print(\"   - The data might not be loaded for the target date\")\n",
        "            print(\"   - There might be a spelling variation in the data\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error executing query: {e}\")\n",
        "        print(\"üí° Troubleshooting tips:\")\n",
        "        print(\"   - Check if the gkg_partitioned table exists and has data\")\n",
        "        print(\"   - Verify the table has V2Persons column\")\n",
        "        print(\"   - Ensure you have the necessary BigQuery permissions\")\n",
        "        return None\n",
        "\n",
        "# Execute the query\n",
        "search_person = \"\"  # Define the search person here\n",
        "cooccurrence_results = query_person_cooccurrence(search_person)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üï∏Ô∏è Network Graph Visualization\n",
        "\n",
        "This cell creates interactive network visualizations from person co-occurrence data using NetworkX and Matplotlib:\n",
        "\n",
        "### **Graph Construction**\n",
        "- **Node Creation**: Each person becomes a network node\n",
        "- **Edge Creation**: Co-occurrence relationships become weighted edges\n",
        "- **Weight Scaling**: Normalizes co-occurrence counts for visualization\n",
        "- **Layout Algorithm**: Uses spring layout for optimal node positioning\n",
        "\n",
        "### **Visualization Features**\n",
        "- **Large Canvas**: 30x20 inch figure for detailed viewing\n",
        "- **Node Styling**: Light blue nodes with configurable sizes\n",
        "- **Edge Styling**: Gray edges with transparency for clarity\n",
        "- **Label Display**: Shows person names on nodes\n",
        "- **Weight Labels**: Displays relationship strengths on significant edges\n",
        "\n",
        "### **Network Analytics**\n",
        "- **Node Count**: Total number of people in the network\n",
        "- **Edge Count**: Total number of relationships\n",
        "- **Average Degree**: Mean connections per person\n",
        "- **Centrality Analysis**: Identifies most connected individuals\n",
        "- **Top Nodes Ranking**: Shows most influential people by connections\n",
        "\n",
        "### **Customization Options**\n",
        "- **Search Focus**: Can highlight specific person's network\n",
        "- **Title Customization**: Dynamic titles based on analysis focus\n",
        "- **Threshold Filtering**: Shows only significant relationships\n",
        "- **Size Scaling**: Adjustable node and edge sizing\n",
        "\n",
        "üìä **Output**: High-resolution network graph with statistical summary and centrality rankings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create network visualization from co-occurrence results\n",
        "def create_person_network_graph(df, search_person=\"Rayner\", title=\"Person Co-occurrence Network\"):\n",
        "    \"\"\"\n",
        "    Create a network graph from the co-occurrence DataFrame\n",
        "    \"\"\"\n",
        "    print(\"üï∏Ô∏è  Creating network graph from co-occurrence data...\")\n",
        "    \n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data available to create network graph\")\n",
        "        return None\n",
        "    \n",
        "    # Create the graph\n",
        "    g = nx.Graph()\n",
        "    \n",
        "    # Add edges with weights based on co-occurrence count\n",
        "    for _, row in df.iterrows():\n",
        "        name1 = row['name1']\n",
        "        name2 = row['name2']\n",
        "        weight = row['pair_count']\n",
        "            \n",
        "        # Add edge with weight (scaled down for visualization)\n",
        "        g.add_edge(name1, name2, weight=weight/10)\n",
        "    \n",
        "    print(f\"‚úÖ Graph created with {g.number_of_nodes()} nodes and {g.number_of_edges()} edges\")\n",
        "    \n",
        "    if not search_person:\n",
        "        search_person = \"All\"\n",
        "    # Create the visualization\n",
        "    plt.figure(figsize=(30, 20))\n",
        "    plt.title(f'GDELT Project: {title}\\nPerson Co-occurrence Network for \"{search_person}\"', \n",
        "              y=0.97, fontsize=20, fontweight='bold')\n",
        "    \n",
        "    # Draw the network\n",
        "    pos = nx.spring_layout(g, k=3, iterations=50)  # Layout algorithm\n",
        "    nx.draw(g, pos, \n",
        "            with_labels=True, \n",
        "            node_color='lightblue',\n",
        "            node_size=500,\n",
        "            font_size=8,\n",
        "            font_weight='bold',\n",
        "            edge_color='gray',\n",
        "            alpha=0.7)\n",
        "    \n",
        "    # Add edge labels for weights (only for top edges to avoid clutter)\n",
        "    edge_labels = {}\n",
        "    for (u, v, d) in g.edges(data=True):\n",
        "        if d['weight'] > 5:  # Only show labels for significant connections\n",
        "            edge_labels[(u, v)] = f\"{d['weight']:.1f}\"\n",
        "    \n",
        "    nx.draw_networkx_edge_labels(g, pos, edge_labels, font_size=6)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print some network statistics\n",
        "    print(f\"\\nüìä Network Statistics:\")\n",
        "    print(f\"   Nodes: {g.number_of_nodes()}\")\n",
        "    print(f\"   Edges: {g.number_of_edges()}\")\n",
        "    print(f\"   Average degree: {sum(dict(g.degree()).values()) / g.number_of_nodes():.2f}\")\n",
        "    \n",
        "    # Find the most connected nodes\n",
        "    degree_centrality = nx.degree_centrality(g)\n",
        "    top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    print(f\"\\nüèÜ Most Connected Nodes:\")\n",
        "    for i, (node, centrality) in enumerate(top_nodes, 1):\n",
        "        print(f\"   {i}. {node}: {centrality:.3f}\")\n",
        "    \n",
        "    return g\n",
        "\n",
        "# Create the network graph from the co-occurrence results\n",
        "network_graph = create_person_network_graph(cooccurrence_results, search_person)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Network Export for Gephi\n",
        "\n",
        "This cell exports the NetworkX graph to GEXF format for advanced analysis in Gephi software:\n",
        "\n",
        "### **Export Process**\n",
        "- **Directory Detection**: Automatically finds suitable output directory\n",
        "- **Fallback Strategy**: Uses multiple directory options (current, home, /tmp)\n",
        "- **GEXF Format**: Exports to Graph Exchange XML Format\n",
        "- **File Management**: Creates organized export directory structure\n",
        "\n",
        "### **Directory Strategy**\n",
        "1. **Current Directory**: Tries notebook's working directory first\n",
        "2. **Home Directory**: Falls back to user's home directory\n",
        "3. **Temporary Directory**: Uses /tmp as last resort\n",
        "4. **Export Folder**: Creates `gephi_exports` subdirectory for organization\n",
        "\n",
        "### **Gephi Integration**\n",
        "- **File Format**: Standard GEXF format compatible with Gephi\n",
        "- **Metadata Preservation**: Maintains node and edge attributes\n",
        "- **Import Instructions**: Provides step-by-step Gephi import guide\n",
        "- **Layout Recommendations**: Suggests Force Atlas 2 algorithm\n",
        "\n",
        "### **Advanced Analysis Capabilities**\n",
        "Once imported into Gephi, you can:\n",
        "- Apply sophisticated layout algorithms\n",
        "- Perform community detection\n",
        "- Calculate advanced centrality measures\n",
        "- Create publication-quality visualizations\n",
        "- Export to various formats (PNG, PDF, SVG)\n",
        "\n",
        "### **File Output**\n",
        "- **Filename**: `gdelt_person_cooccurrence.gexf`\n",
        "- **Location**: `~/gephi_exports/` (or alternative directory)\n",
        "- **Format**: XML-based graph exchange format\n",
        "\n",
        "üéØ **Purpose**: Enables professional-grade network analysis and visualization in Gephi software.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export NetworkX graph to Gephi file formats\n",
        "\n",
        "def export_graph_to_gephi(graph, filename_prefix=\"gdelt_person_network\"):\n",
        "    \"\"\"\n",
        "    Export NetworkX graph to GEXF format for Gephi\n",
        "    \"\"\"\n",
        "    if graph is None:\n",
        "        print(\"‚ùå No graph available to export\")\n",
        "        return None\n",
        "    \n",
        "    print(\"üìÅ Exporting NetworkX graph to GEXF format...\")\n",
        "    print(f\"   Graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "    \n",
        "    # Use a more robust approach to get the output directory\n",
        "    # Try multiple fallback options\n",
        "    possible_dirs = []\n",
        "    \n",
        "    # Option 1: Try to get current working directory\n",
        "    try:\n",
        "        current_dir = Path.cwd()\n",
        "        possible_dirs.append(current_dir)\n",
        "        print(f\"üìÇ Found current directory: {current_dir}\")\n",
        "    except (FileNotFoundError, OSError) as e:\n",
        "        print(f\"‚ö†Ô∏è  Current directory not accessible: {e}\")\n",
        "    \n",
        "    # Option 2: Use the notebook directory (where this notebook is located)\n",
        "    try:\n",
        "        notebook_dir = Path(__file__).parent if '__file__' in globals() else None\n",
        "        if notebook_dir and notebook_dir.exists():\n",
        "            possible_dirs.append(notebook_dir)\n",
        "            print(f\"üìÇ Found notebook directory: {notebook_dir}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Option 3: Use home directory as fallback\n",
        "    try:\n",
        "        home_dir = Path.home()\n",
        "        possible_dirs.append(home_dir)\n",
        "        print(f\"üìÇ Using home directory as fallback: {home_dir}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Option 4: Use /tmp as last resort\n",
        "    try:\n",
        "        tmp_dir = Path(\"/tmp\")\n",
        "        if tmp_dir.exists():\n",
        "            possible_dirs.append(tmp_dir)\n",
        "            print(f\"üìÇ Using /tmp directory as last resort: {tmp_dir}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Select the first available directory\n",
        "    if not possible_dirs:\n",
        "        print(\"‚ùå No suitable directory found for export\")\n",
        "        return None\n",
        "    \n",
        "    base_dir = possible_dirs[0]\n",
        "    output_dir = base_dir / \"gephi_exports\"\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    try:\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        print(f\"üìÇ Output directory: {output_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not create output directory: {e}\")\n",
        "        # Fall back to base directory\n",
        "        output_dir = base_dir\n",
        "        print(f\"üìÇ Using base directory: {output_dir}\")\n",
        "    \n",
        "    try:\n",
        "        # Export to GEXF format only\n",
        "        gexf_filename = output_dir / f\"{filename_prefix}.gexf\"\n",
        "        nx.write_gexf(graph, str(gexf_filename))\n",
        "        print(f\"‚úÖ GEXF file exported: {gexf_filename}\")\n",
        "        \n",
        "        print(f\"\\nüéâ Graph export completed successfully!\")\n",
        "        print(f\"üìÇ File created: {gexf_filename}\")\n",
        "        \n",
        "        print(f\"\\nüí° To import into Gephi:\")\n",
        "        print(f\"   1. Open Gephi\")\n",
        "        print(f\"   2. File ‚Üí Open ‚Üí Select '{gexf_filename.name}'\")\n",
        "        print(f\"   3. Choose appropriate layout algorithm (e.g., Force Atlas 2)\")\n",
        "        print(f\"   4. Adjust node sizes and colors as needed\")\n",
        "        \n",
        "        return str(gexf_filename)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error exporting graph: {e}\")\n",
        "        print(f\"üí° Base directory: {base_dir}\")\n",
        "        print(f\"üí° Output directory: {output_dir}\")\n",
        "        print(f\"ÔøΩÔøΩ Directory exists: {output_dir.exists()}\")\n",
        "        print(f\"üí° Directory writable: {os.access(output_dir, os.W_OK)}\")\n",
        "        return None\n",
        "\n",
        "# Export the network graph to GEXF format\n",
        "if 'network_graph' in locals() and network_graph is not None:\n",
        "    export_file = export_graph_to_gephi(network_graph, \"gdelt_person_cooccurrence\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No network graph available. Please run the network creation cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Graph Database Schema Creation\n",
        "\n",
        "This cell creates a normalized graph database schema optimized for GDELT network analysis:\n",
        "\n",
        "### **Schema Design**\n",
        "The schema implements a proper graph database structure with dedicated tables for:\n",
        "\n",
        "#### **Node Tables (Entities)**\n",
        "- **person**: Individual people with name parsing and mention statistics\n",
        "- **organization**: Organizations with type classification and geographic info\n",
        "- **location**: Geographic locations with coordinates and country codes\n",
        "- **event**: GDELT events with categorization and descriptions\n",
        "- **article**: Article metadata with tone scores and publication info\n",
        "\n",
        "#### **Relationship Tables (Edges)**\n",
        "- **person_cooccurrence**: Person-to-person relationships with strength scores\n",
        "- **person_organization**: Person-organization affiliations\n",
        "- **person_location**: Person-location associations\n",
        "- **event_participant**: Event participation relationships\n",
        "\n",
        "### **Schema Features**\n",
        "- **UUID Primary Keys**: Unique identifiers for all entities\n",
        "- **Clustering**: Optimized for query performance on key columns\n",
        "- **Temporal Tracking**: First/last seen dates for relationship evolution\n",
        "- **Array Fields**: Supports multiple values (name variations, article IDs)\n",
        "- **Metadata Fields**: Creation and update timestamps\n",
        "- **Flexible Relationships**: Support for different relationship types\n",
        "\n",
        "### **Performance Optimizations**\n",
        "- **Clustered Tables**: Organized by most frequently queried columns\n",
        "- **No Partitioning**: Simplified structure to avoid BigQuery complexity\n",
        "- **Efficient Joins**: Designed for fast relationship queries\n",
        "- **Index-Friendly**: Column ordering optimized for BigQuery\n",
        "\n",
        "üéØ **Purpose**: Creates a foundation for sophisticated graph queries and analysis on GDELT data.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è  Creating Graph Schema Tables (Fixed Version)...\n",
            "üìä Project: graph-demo-471710\n",
            "üóÑÔ∏è  Dataset: gdelt\n",
            "----------------------------------------------------------------------\n",
            "‚úÖ BigQuery client created\n",
            "\n",
            "üìù Creating table: person\n",
            "‚úÖ Table 'person' created successfully\n",
            "\n",
            "üìù Creating table: organization\n",
            "‚úÖ Table 'organization' created successfully\n",
            "\n",
            "üìù Creating table: location\n",
            "‚úÖ Table 'location' created successfully\n",
            "\n",
            "üìù Creating table: event\n",
            "‚úÖ Table 'event' created successfully\n",
            "\n",
            "üìù Creating table: person_cooccurrence\n",
            "‚úÖ Table 'person_cooccurrence' created successfully\n",
            "\n",
            "üìù Creating table: person_organization\n",
            "‚úÖ Table 'person_organization' created successfully\n",
            "\n",
            "üìù Creating table: person_location\n",
            "‚úÖ Table 'person_location' created successfully\n",
            "\n",
            "üìù Creating table: event_participant\n",
            "‚úÖ Table 'event_participant' created successfully\n",
            "\n",
            "üìù Creating table: article\n",
            "‚úÖ Table 'article' created successfully\n",
            "\n",
            "======================================================================\n",
            "üìä GRAPH SCHEMA CREATION SUMMARY (FIXED)\n",
            "======================================================================\n",
            "‚úÖ Successfully created: 9/9 tables\n",
            "‚è≠Ô∏è  Already existed: 0/9 tables\n",
            "‚ùå Errors: 0/9 tables\n",
            "\n",
            "Detailed results:\n",
            "  ‚úÖ person: Created\n",
            "  ‚úÖ organization: Created\n",
            "  ‚úÖ location: Created\n",
            "  ‚úÖ event: Created\n",
            "  ‚úÖ person_cooccurrence: Created\n",
            "  ‚úÖ person_organization: Created\n",
            "  ‚úÖ person_location: Created\n",
            "  ‚úÖ event_participant: Created\n",
            "  ‚úÖ article: Created\n",
            "\n",
            "üéâ Graph schema tables creation completed!\n",
            "\n",
            "üí° Next steps:\n",
            "   1. Populate node tables with data from gkg_partitioned\n",
            "   2. Create relationship tables from co-occurrence analysis\n",
            "   3. Run graph queries on the normalized schema\n"
          ]
        }
      ],
      "source": [
        "# Create Graph Schema Tables (Fixed - No Partitioning)\n",
        "\n",
        "def create_graph_schema_tables_fixed():\n",
        "    \"\"\"\n",
        "    Create normalized graph schema tables for GDELT data analysis.\n",
        "    Fixed version without partitioning to avoid BigQuery errors.\n",
        "    \"\"\"\n",
        "    print(\"üèóÔ∏è  Creating Graph Schema Tables (Fixed Version)...\")\n",
        "    print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "    print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        # Define all table creation queries (without partitioning)\n",
        "        table_definitions = {\n",
        "            \"person\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.person` (\n",
        "              person_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the person.\"),\n",
        "              name STRING NOT NULL OPTIONS(description=\"The common name of the person.\"),\n",
        "              first_name STRING,\n",
        "              last_name STRING,\n",
        "              full_name STRING,\n",
        "              name_variations ARRAY<STRING> OPTIONS(description=\"Known variations or aliases of the person's name.\"),\n",
        "              first_seen_date DATE OPTIONS(description=\"The date the person was first mentioned.\"),\n",
        "              last_seen_date DATE OPTIONS(description=\"The date the person was last mentioned.\"),\n",
        "              total_mentions INT64 OPTIONS(description=\"A total count of the person's mentions.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (person_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY person_id, name\n",
        "            OPTIONS(\n",
        "              description=\"Node table containing master list of all identified individuals.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"organization\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.organization` (\n",
        "              org_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the organization.\"),\n",
        "              name STRING NOT NULL OPTIONS(description=\"The common name of the organization.\"),\n",
        "              org_type STRING OPTIONS(description=\"The type or category of the organization.\"),\n",
        "              country_code STRING OPTIONS(description=\"ISO country code where the organization is based.\"),\n",
        "              first_seen_date DATE OPTIONS(description=\"The date the organization was first mentioned.\"),\n",
        "              last_seen_date DATE OPTIONS(description=\"The date the organization was last mentioned.\"),\n",
        "              total_mentions INT64 OPTIONS(description=\"A total count of the organization's mentions.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (org_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY org_id, name\n",
        "            OPTIONS(\n",
        "              description=\"Node table containing master list of all identified organizations.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"location\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.location` (\n",
        "              location_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the location.\"),\n",
        "              name STRING NOT NULL OPTIONS(description=\"The common name of the location.\"),\n",
        "              location_type STRING OPTIONS(description=\"The type of location (city, country, region, etc.).\"),\n",
        "              country_code STRING OPTIONS(description=\"ISO country code for the location.\"),\n",
        "              latitude FLOAT64 OPTIONS(description=\"Geographic latitude coordinate.\"),\n",
        "              longitude FLOAT64 OPTIONS(description=\"Geographic longitude coordinate.\"),\n",
        "              first_seen_date DATE OPTIONS(description=\"The date the location was first mentioned.\"),\n",
        "              last_seen_date DATE OPTIONS(description=\"The date the location was last mentioned.\"),\n",
        "              total_mentions INT64 OPTIONS(description=\"A total count of the location's mentions.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (location_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY location_id, name\n",
        "            OPTIONS(\n",
        "              description=\"Node table containing master list of all identified geographic locations.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"event\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.event` (\n",
        "              event_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the event.\"),\n",
        "              event_code STRING OPTIONS(description=\"GDELT event code classification.\"),\n",
        "              event_description STRING OPTIONS(description=\"Human-readable description of the event.\"),\n",
        "              event_category STRING OPTIONS(description=\"High-level category classification of the event.\"),\n",
        "              first_seen_date DATE OPTIONS(description=\"The date the event was first mentioned.\"),\n",
        "              last_seen_date DATE OPTIONS(description=\"The date the event was last mentioned.\"),\n",
        "              total_mentions INT64 OPTIONS(description=\"A total count of the event's mentions.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (event_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY event_id, event_code\n",
        "            OPTIONS(\n",
        "              description=\"Node table containing master list of all identified events.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"person_cooccurrence\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.person_cooccurrence` (\n",
        "              relationship_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for this specific co-occurrence.\"),\n",
        "              person1_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing person_id in the person table.\"),\n",
        "              person2_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing person_id in the person table.\"),\n",
        "              cooccurrence_count INT64 OPTIONS(description=\"The number of times these two people were mentioned together.\"),\n",
        "              first_cooccurrence_date DATE OPTIONS(description=\"The date of the first joint mention.\"),\n",
        "              last_cooccurrence_date DATE OPTIONS(description=\"The date of the most recent joint mention.\"),\n",
        "              article_ids ARRAY<STRING> OPTIONS(description=\"A list of article IDs where the co-occurrence was found.\"),\n",
        "              themes ARRAY<STRING> OPTIONS(description=\"A list of themes associated with their joint mentions.\"),\n",
        "              themes_summary STRING OPTIONS(description=\"A summary of the common themes in their co-occurrence.\"),\n",
        "              strength_score FLOAT64 OPTIONS(description=\"A calculated score representing the strength of the relationship.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (relationship_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_person1 FOREIGN KEY (person1_id) REFERENCES `{project_id}.{dataset}.person` (person_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_person2 FOREIGN KEY (person2_id) REFERENCES `{project_id}.{dataset}.person` (person_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY person1_id, person2_id\n",
        "            OPTIONS(\n",
        "              description=\"Edge table storing the relationships (co-occurrences) between individuals from the person table.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"person_organization\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.person_organization` (\n",
        "              relationship_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the person-organization relationship.\"),\n",
        "              person_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing person.person_id.\"),\n",
        "              org_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing organization.org_id.\"),\n",
        "              relationship_type STRING OPTIONS(description=\"Type of relationship between the person and organization.\"),\n",
        "              mention_count INT64 OPTIONS(description=\"Number of mentions linking this person to the organization.\"),\n",
        "              first_mention_date DATE OPTIONS(description=\"Date of the first mention linking them.\"),\n",
        "              last_mention_date DATE OPTIONS(description=\"Date of the most recent mention linking them.\"),\n",
        "              article_ids ARRAY<STRING> OPTIONS(description=\"List of article IDs where this relationship was mentioned.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (relationship_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_po_person FOREIGN KEY (person_id) REFERENCES `{project_id}.{dataset}.person` (person_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_po_org FOREIGN KEY (org_id) REFERENCES `{project_id}.{dataset}.organization` (org_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY person_id, org_id\n",
        "            OPTIONS(\n",
        "              description=\"Edge table for person-to-organization affiliations and mentions.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"person_location\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.person_location` (\n",
        "              relationship_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the person-location relationship.\"),\n",
        "              person_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing person.person_id.\"),\n",
        "              location_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing location.location_id.\"),\n",
        "              relationship_type STRING OPTIONS(description=\"Type of relationship between the person and location.\"),\n",
        "              mention_count INT64 OPTIONS(description=\"Number of mentions linking this person to the location.\"),\n",
        "              first_mention_date DATE OPTIONS(description=\"Date of the first mention linking them.\"),\n",
        "              last_mention_date DATE OPTIONS(description=\"Date of the most recent mention linking them.\"),\n",
        "              article_ids ARRAY<STRING> OPTIONS(description=\"List of article IDs where this relationship was mentioned.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (relationship_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_pl_person FOREIGN KEY (person_id) REFERENCES `{project_id}.{dataset}.person` (person_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_pl_location FOREIGN KEY (location_id) REFERENCES `{project_id}.{dataset}.location` (location_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY person_id, location_id\n",
        "            OPTIONS(\n",
        "              description=\"Edge table for person-to-location associations and mentions.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"event_participant\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.event_participant` (\n",
        "              relationship_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the event-participant relationship.\"),\n",
        "              event_id STRING NOT NULL OPTIONS(description=\"Logical Foreign Key referencing event.event_id.\"),\n",
        "              participant_id STRING NOT NULL OPTIONS(description=\"Identifier of the participant (person, organization, or location).\"),\n",
        "              participant_type STRING OPTIONS(description=\"Type of participant: PERSON, ORGANIZATION, or LOCATION.\"),\n",
        "              role STRING OPTIONS(description=\"Role of the participant in the event (e.g., initiator, target).\"),\n",
        "              mention_count INT64 OPTIONS(description=\"Number of mentions linking this participant to the event.\"),\n",
        "              first_mention_date DATE OPTIONS(description=\"Date of the first mention linking them.\"),\n",
        "              last_mention_date DATE OPTIONS(description=\"Date of the most recent mention linking them.\"),\n",
        "              article_ids ARRAY<STRING> OPTIONS(description=\"List of article IDs where this participation was mentioned.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was last updated.\"),\n",
        "              PRIMARY KEY (relationship_id) NOT ENFORCED,\n",
        "              CONSTRAINT fk_ep_event FOREIGN KEY (event_id) REFERENCES `{project_id}.{dataset}.event` (event_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY event_id, participant_id\n",
        "            OPTIONS(\n",
        "              description=\"Edge table for event participation by persons, organizations, or locations.\"\n",
        "            )\n",
        "            \"\"\",\n",
        "            \n",
        "            \"article\": \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS `{project_id}.{dataset}.article` (\n",
        "              article_id STRING NOT NULL OPTIONS(description=\"Logical Primary Key. Unique identifier for the article.\"),\n",
        "              gkg_record_id STRING OPTIONS(description=\"Reference to the original GDELT GKG record.\"),\n",
        "              url STRING OPTIONS(description=\"URL of the original article.\"),\n",
        "              title STRING OPTIONS(description=\"Title of the article.\"),\n",
        "              publish_date DATE OPTIONS(description=\"Date when the article was published.\"),\n",
        "              source_name STRING OPTIONS(description=\"Name of the media source.\"),\n",
        "              language STRING OPTIONS(description=\"Language code of the article.\"),\n",
        "              tone_score FLOAT64 OPTIONS(description=\"Sentiment tone score of the article.\"),\n",
        "              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP() OPTIONS(description=\"Timestamp when the record was created.\"),\n",
        "              PRIMARY KEY (article_id) NOT ENFORCED\n",
        "            )\n",
        "            CLUSTER BY article_id, publish_date\n",
        "            OPTIONS(\n",
        "              description=\"Node table containing master list of all articles from GDELT data.\"\n",
        "            )\n",
        "            \"\"\"\n",
        "        }\n",
        "        \n",
        "        # Create tables\n",
        "        results = {}\n",
        "        for table_name, query_template in table_definitions.items():\n",
        "            print(f\"\\nüìù Creating table: {table_name}\")\n",
        "            \n",
        "            try:\n",
        "                # Format the query with project and dataset\n",
        "                query = query_template.format(\n",
        "                    project_id=GCP_PROJECT_ID,\n",
        "                    dataset=BIGQUERY_DATASET\n",
        "                )\n",
        "                \n",
        "                # Execute the query\n",
        "                query_job = client.query(query, location=\"US-CENTRAL1\")\n",
        "                query_job.result()  # Wait for completion\n",
        "                \n",
        "                print(f\"‚úÖ Table '{table_name}' created successfully\")\n",
        "                results[table_name] = \"success\"\n",
        "                \n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                if \"already exists\" in error_msg.lower() or \"409\" in error_msg:\n",
        "                    print(f\"‚è≠Ô∏è  Table '{table_name}' already exists\")\n",
        "                    results[table_name] = \"already_exists\"\n",
        "                else:\n",
        "                    print(f\"‚ùå Error creating table '{table_name}': {e}\")\n",
        "                    results[table_name] = f\"error: {e}\"\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"üìä GRAPH SCHEMA CREATION SUMMARY (FIXED)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        success_count = sum(1 for r in results.values() if r == \"success\")\n",
        "        exists_count = sum(1 for r in results.values() if r == \"already_exists\")\n",
        "        error_count = sum(1 for r in results.values() if r.startswith(\"error\"))\n",
        "        \n",
        "        print(f\"‚úÖ Successfully created: {success_count}/{len(table_definitions)} tables\")\n",
        "        print(f\"‚è≠Ô∏è  Already existed: {exists_count}/{len(table_definitions)} tables\")\n",
        "        print(f\"‚ùå Errors: {error_count}/{len(table_definitions)} tables\")\n",
        "        \n",
        "        print(f\"\\nDetailed results:\")\n",
        "        for table_name, result in results.items():\n",
        "            if result == \"success\":\n",
        "                print(f\"  ‚úÖ {table_name}: Created\")\n",
        "            elif result == \"already_exists\":\n",
        "                print(f\"  ‚è≠Ô∏è  {table_name}: Already exists\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {table_name}: {result}\")\n",
        "        \n",
        "        print(f\"\\nüéâ Graph schema tables creation completed!\")\n",
        "        print(f\"\\nüí° Next steps:\")\n",
        "        print(f\"   1. Populate node tables with data from gkg_partitioned\")\n",
        "        print(f\"   2. Create relationship tables from co-occurrence analysis\")\n",
        "        print(f\"   3. Run graph queries on the normalized schema\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Critical error during table creation: {e}\")\n",
        "        print(\"üí° Troubleshooting tips:\")\n",
        "        print(\"   - Check if you have BigQuery admin permissions\")\n",
        "        print(\"   - Verify the dataset exists\")\n",
        "        print(\"   - Ensure BigQuery API is enabled\")\n",
        "        return None\n",
        "\n",
        "# Execute the fixed table creation\n",
        "schema_results_fixed = create_graph_schema_tables_fixed()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• Person Entity Population\n",
        "\n",
        "This cell populates the person table with cleaned and deduplicated person data from GDELT:\n",
        "\n",
        "### **Data Processing Pipeline**\n",
        "1. **Name Extraction**: Extracts person names from V2Persons field\n",
        "2. **Name Cleaning**: Removes titles, suffixes, and standardizes format\n",
        "3. **Name Parsing**: Separates first and last names using string splitting\n",
        "4. **Deduplication**: Ensures each unique person appears only once\n",
        "5. **Mention Counting**: Calculates total mentions across all articles\n",
        "\n",
        "### **Name Cleaning Rules**\n",
        "- **Suffix Removal**: Strips everything after commas (titles, descriptions)\n",
        "- **Prefix Removal**: Removes common prefixes like \"A \" \n",
        "- **Standardization**: Normalizes spacing and formatting\n",
        "- **Validation**: Filters out empty or invalid names\n",
        "\n",
        "### **Data Enrichment**\n",
        "- **UUID Generation**: Creates unique identifiers for each person\n",
        "- **Name Components**: Extracts first_name and last_name fields\n",
        "- **Full Name**: Preserves complete name for display\n",
        "- **Statistics**: Counts total mentions across all articles\n",
        "- **Temporal Data**: Records first and last seen dates\n",
        "\n",
        "### **Quality Assurance**\n",
        "- **Duplicate Prevention**: Uses DISTINCT to avoid person duplicates\n",
        "- **Null Filtering**: Excludes empty or null person entries\n",
        "- **Validation**: Ensures clean_name is not empty after processing\n",
        "\n",
        "üìä **Expected Output**: ~195,000 unique persons with cleaned names and mention statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Populate Person Table\n",
        "\n",
        "def populate_person_table():\n",
        "    \"\"\"\n",
        "    Populate the person table with unique persons from GDELT data.\n",
        "    Removes duplicates and properly parses first/last names.\n",
        "    \"\"\"\n",
        "    print(\"üë• Populating Person Table...\")\n",
        "    print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "    print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        person_query = f\"\"\"\n",
        "        INSERT INTO `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person` \n",
        "        (person_id, name, first_name, last_name, full_name, first_seen_date, last_seen_date, total_mentions)\n",
        "        WITH CleanedPersons AS (\n",
        "          SELECT DISTINCT\n",
        "            REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '') as clean_name,\n",
        "            CASE \n",
        "              WHEN ARRAY_LENGTH(SPLIT(REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', ''), ' ')) > 0 \n",
        "              THEN SPLIT(REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', ''), ' ')[OFFSET(0)]\n",
        "              ELSE NULL\n",
        "            END as first_name,\n",
        "            CASE \n",
        "              WHEN ARRAY_LENGTH(SPLIT(REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', ''), ' ')) > 1 \n",
        "              THEN SPLIT(REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', ''), ' ')[OFFSET(ARRAY_LENGTH(SPLIT(REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', ''), ' ')) - 1)]\n",
        "              ELSE NULL\n",
        "            END as last_name,\n",
        "            REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '') as full_name\n",
        "          FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.gkg_partitioned`,\n",
        "          UNNEST(SPLIT(V2Persons, ';')) AS person\n",
        "          WHERE V2Persons IS NOT NULL AND V2Persons != '' AND REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '') != ''\n",
        "        ),\n",
        "        PersonCounts AS (\n",
        "          SELECT \n",
        "            REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '') as clean_name,\n",
        "            COUNT(*) as total_mentions\n",
        "          FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.gkg_partitioned`,\n",
        "          UNNEST(SPLIT(V2Persons, ';')) AS person\n",
        "          WHERE V2Persons IS NOT NULL AND V2Persons != '' AND REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '') != ''\n",
        "          GROUP BY REGEXP_REPLACE(REGEXP_REPLACE(person, r',.*', ''), r'^A ', '')\n",
        "        )\n",
        "        SELECT \n",
        "          GENERATE_UUID() as person_id,\n",
        "          cp.clean_name as name,\n",
        "          cp.first_name,\n",
        "          cp.last_name,\n",
        "          cp.full_name,\n",
        "          CURRENT_DATE() as first_seen_date,\n",
        "          CURRENT_DATE() as last_seen_date,\n",
        "          pc.total_mentions\n",
        "        FROM CleanedPersons cp\n",
        "        JOIN PersonCounts pc ON cp.clean_name = pc.clean_name\n",
        "        \"\"\"\n",
        "        \n",
        "        query_job = client.query(person_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        print(\"‚úÖ Person table populated successfully\")\n",
        "        \n",
        "        # Verify the data\n",
        "        verification_query = f\"SELECT COUNT(*) as count FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person`\"\n",
        "        query_job = client.query(verification_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        for row in result:\n",
        "            print(f\"üìä Total persons: {row.count:,}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error populating person table: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute person table population\n",
        "person_success = populate_person_table()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì∞ Article Metadata Population\n",
        "\n",
        "This cell populates the article table with metadata from GDELT Global Knowledge Graph records:\n",
        "\n",
        "### **Article Data Extraction**\n",
        "- **Unique IDs**: Generates UUID for each article record\n",
        "- **GKG Record ID**: Links to original GDELT GKG record\n",
        "- **URL Mapping**: Uses DocumentIdentifier as article URL\n",
        "- **Source Information**: Extracts source collection identifiers\n",
        "\n",
        "### **Tone Analysis**\n",
        "- **Tone Score Parsing**: Extracts numeric tone values from V2Tone field\n",
        "- **Sentiment Indication**: Tone scores indicate article sentiment\n",
        "  - Positive values: Positive tone\n",
        "  - Negative values: Negative tone\n",
        "  - Values near zero: Neutral tone\n",
        "\n",
        "### **Data Fields**\n",
        "- **article_id**: Unique identifier (UUID)\n",
        "- **gkg_record_id**: Original GDELT record reference\n",
        "- **url**: Article web address\n",
        "- **publish_date**: Publication date (set to current date)\n",
        "- **source_name**: Media source identifier\n",
        "- **tone_score**: Numerical sentiment score\n",
        "\n",
        "### **Limitations & Notes**\n",
        "- Uses current date as placeholder for publish_date\n",
        "- Some fields use DocumentIdentifier as placeholder\n",
        "- Tone score is first value from comma-separated V2Tone field\n",
        "- Designed for demonstration purposes with simplified mapping\n",
        "\n",
        "üìä **Expected Output**: ~382,000 article records with basic metadata and tone scores.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Populate Article Table\n",
        "\n",
        "def populate_article_table():\n",
        "    \"\"\"\n",
        "    Populate the article table with article metadata from GDELT data.\n",
        "    \"\"\"\n",
        "    print(\"üì∞ Populating Article Table...\")\n",
        "    print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "    print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        article_query = f\"\"\"\n",
        "        INSERT INTO `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.article` \n",
        "        (article_id, gkg_record_id, url, title, publish_date, source_name, language, tone_score)\n",
        "        SELECT \n",
        "          GENERATE_UUID() as article_id,\n",
        "          GKGRECORDID as gkg_record_id,\n",
        "          DocumentIdentifier as url,\n",
        "          V2Tone as title,  -- Using V2Tone as placeholder for title\n",
        "          CURRENT_DATE() as publish_date,\n",
        "          CAST(SourceCollectionIdentifier AS STRING) as source_name,\n",
        "          DocumentIdentifier as language,  -- Using DocumentIdentifier as placeholder\n",
        "          SAFE_CAST(SPLIT(V2Tone, ',')[OFFSET(0)] AS FLOAT64) as tone_score\n",
        "        FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.gkg_partitioned`\n",
        "        WHERE GKGRECORDID IS NOT NULL\n",
        "        \"\"\"\n",
        "        \n",
        "        query_job = client.query(article_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        print(\"‚úÖ Article table populated successfully\")\n",
        "        \n",
        "        # Verify the data\n",
        "        verification_query = f\"SELECT COUNT(*) as count FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.article`\"\n",
        "        query_job = client.query(verification_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        for row in result:\n",
        "            print(f\"üìä Total articles: {row.count:,}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error populating article table: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute article table population\n",
        "article_success = populate_article_table()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Person Relationship Population\n",
        "\n",
        "This cell creates the person co-occurrence relationship table by analyzing which people appear together in articles:\n",
        "\n",
        "### **Relationship Detection Process**\n",
        "1. **Person Pair Generation**: Creates all possible person combinations within each article\n",
        "2. **Name Standardization**: Applies same cleaning rules as person table\n",
        "3. **Duplicate Prevention**: Uses `a < b` condition to avoid duplicate pairs\n",
        "4. **Foreign Key Matching**: Links to person table using cleaned names\n",
        "5. **Aggregation**: Counts co-occurrence frequencies across all articles\n",
        "\n",
        "### **Relationship Metrics**\n",
        "- **Cooccurrence Count**: Total number of shared article mentions\n",
        "- **Temporal Tracking**: First and last co-occurrence dates\n",
        "- **Article References**: Array of article IDs containing both people\n",
        "- **Relationship Strength**: Implicit in co-occurrence frequency\n",
        "\n",
        "### **Data Quality Features**\n",
        "- **Self-Pair Exclusion**: Prevents person from being paired with themselves\n",
        "- **Empty Name Filtering**: Excludes invalid or empty person names\n",
        "- **Referential Integrity**: Ensures both people exist in person table\n",
        "- **Unique Relationships**: Each person pair appears only once\n",
        "\n",
        "### **Performance Considerations**\n",
        "- **Cross Join**: Computationally intensive operation on large datasets\n",
        "- **Aggregation**: Groups by person IDs for relationship counting\n",
        "- **Array Collection**: Gathers all article IDs for each relationship\n",
        "\n",
        "### **Schema Integration**\n",
        "- Links to person table via foreign keys\n",
        "- Supports graph traversal queries\n",
        "- Enables network analysis and visualization\n",
        "- Foundation for influence and community detection\n",
        "\n",
        "üìä **Expected Output**: ~1.2 million person-to-person relationships with co-occurrence statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Populating Person Co-occurrence Table...\n",
            "üìä Project: graph-demo-471710\n",
            "üóÑÔ∏è  Dataset: gdelt\n",
            "--------------------------------------------------\n",
            "‚úÖ BigQuery client created\n",
            "‚úÖ Person cooccurrence table populated successfully\n",
            "üìä Total co-occurrence relationships: 1,173,279\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Populate Person Co-occurrence Table\n",
        "\n",
        "def populate_person_cooccurrence_table():\n",
        "    \"\"\"\n",
        "    Populate the person_cooccurrence table with relationships between people\n",
        "    who appear together in the same articles.\n",
        "    \"\"\"\n",
        "    print(\"üîó Populating Person Co-occurrence Table...\")\n",
        "    print(f\"üìä Project: {GCP_PROJECT_ID}\")\n",
        "    print(f\"üóÑÔ∏è  Dataset: {BIGQUERY_DATASET}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "        print(\"‚úÖ BigQuery client created\")\n",
        "        \n",
        "        cooccurrence_query = f\"\"\"\n",
        "        INSERT INTO `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person_cooccurrence` \n",
        "        (relationship_id, person1_id, person2_id, cooccurrence_count, first_cooccurrence_date, last_cooccurrence_date, article_ids, themes)\n",
        "        WITH PersonPairs AS (\n",
        "          SELECT DISTINCT\n",
        "            g.GKGRECORDID,\n",
        "            g.V2Themes,\n",
        "            REGEXP_REPLACE(REGEXP_REPLACE(a, r',.*', ''), r'^A ', '') AS name1,\n",
        "            REGEXP_REPLACE(REGEXP_REPLACE(b, r',.*', ''), r'^A ', '') AS name2,\n",
        "            PARSE_DATE('%Y%m%d', SUBSTR(CAST(DATE AS STRING), 1, 8)) as cooccurrence_date\n",
        "          FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.gkg_partitioned` g,\n",
        "          UNNEST(SPLIT(V2Persons, ';')) AS a,\n",
        "          UNNEST(SPLIT(V2Persons, ';')) AS b\n",
        "          WHERE a < b  -- Avoid duplicates and self-pairs\n",
        "            AND REGEXP_REPLACE(REGEXP_REPLACE(a, r',.*', ''), r'^A ', '') != ''\n",
        "            AND REGEXP_REPLACE(REGEXP_REPLACE(b, r',.*', ''), r'^A ', '') != ''\n",
        "        ),\n",
        "        PersonCooccurrence AS (\n",
        "          SELECT \n",
        "            p1.person_id as person1_id,\n",
        "            p2.person_id as person2_id,\n",
        "            COUNT(*) as cooccurrence_count,\n",
        "            MIN(cooccurrence_date) as first_cooccurrence_date,\n",
        "            MAX(cooccurrence_date) as last_cooccurrence_date,\n",
        "            ARRAY_AGG(DISTINCT GKGRECORDID) as article_ids\n",
        "          FROM PersonPairs pp\n",
        "          JOIN `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person` p1 ON pp.name1 = p1.name\n",
        "          JOIN `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person` p2 ON pp.name2 = p2.name\n",
        "          GROUP BY p1.person_id, p2.person_id\n",
        "        ),\n",
        "        PersonThemes AS (\n",
        "          SELECT \n",
        "            p1.person_id as person1_id,\n",
        "            p2.person_id as person2_id,\n",
        "            ARRAY_AGG(\n",
        "              theme\n",
        "              ORDER BY theme_count DESC\n",
        "              LIMIT 20\n",
        "            ) as themes\n",
        "          FROM (\n",
        "            SELECT \n",
        "              pp.name1,\n",
        "              pp.name2,\n",
        "              REGEXP_REPLACE(theme, r',.*', '') as theme,\n",
        "              COUNT(*) as theme_count\n",
        "            FROM PersonPairs pp,\n",
        "            UNNEST(SPLIT(V2Themes, ';')) AS theme\n",
        "            WHERE theme IS NOT NULL AND theme != ''\n",
        "            GROUP BY pp.name1, pp.name2, REGEXP_REPLACE(theme, r',.*', '')\n",
        "          ) theme_counts\n",
        "          JOIN `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person` p1 ON theme_counts.name1 = p1.name\n",
        "          JOIN `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person` p2 ON theme_counts.name2 = p2.name\n",
        "          GROUP BY p1.person_id, p2.person_id\n",
        "        )\n",
        "        SELECT \n",
        "          GENERATE_UUID() as relationship_id,\n",
        "          pc.person1_id,\n",
        "          pc.person2_id,\n",
        "          pc.cooccurrence_count,\n",
        "          pc.first_cooccurrence_date,\n",
        "          pc.last_cooccurrence_date,\n",
        "          pc.article_ids,\n",
        "          COALESCE(pt.themes, []) as themes\n",
        "        FROM PersonCooccurrence pc\n",
        "        LEFT JOIN PersonThemes pt ON pc.person1_id = pt.person1_id AND pc.person2_id = pt.person2_id\n",
        "        WHERE pc.person1_id != pc.person2_id  -- Ensure person1 ‚â† person2\n",
        "        \"\"\"\n",
        "        \n",
        "        query_job = client.query(cooccurrence_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        print(\"‚úÖ Person cooccurrence table populated successfully\")\n",
        "        \n",
        "        # Verify the data\n",
        "        verification_query = f\"SELECT COUNT(*) as count FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.person_cooccurrence`\"\n",
        "        query_job = client.query(verification_query, location=\"US-CENTRAL1\")\n",
        "        result = query_job.result()\n",
        "        for row in result:\n",
        "            print(f\"üìä Total co-occurrence relationships: {row.count:,}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error populating person_cooccurrence table: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute person cooccurrence table population\n",
        "cooccurrence_success = populate_person_cooccurrence_table()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create network visualization from co-occurrence results\n",
        "def create_person_network_graph(df, search_person=\"Rayner\", title=\"Person Co-occurrence Network\"):\n",
        "    \"\"\"\n",
        "    Create a network graph from the co-occurrence DataFrame\n",
        "    \"\"\"\n",
        "    print(\"üï∏Ô∏è  Creating network graph from co-occurrence data...\")\n",
        "    \n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data available to create network graph\")\n",
        "        return None\n",
        "    \n",
        "    # Create the graph\n",
        "    g = nx.Graph()\n",
        "    \n",
        "    # Add edges with weights based on co-occurrence count\n",
        "    for _, row in df.iterrows():\n",
        "        name1 = row['name1']\n",
        "        name2 = row['name2']\n",
        "        weight = row['pair_count']\n",
        "            \n",
        "        # Add edge with weight (scaled down for visualization)\n",
        "        g.add_edge(name1, name2, weight=weight/10)\n",
        "    \n",
        "    print(f\"‚úÖ Graph created with {g.number_of_nodes()} nodes and {g.number_of_edges()} edges\")\n",
        "    \n",
        "    if not search_person:\n",
        "        search_person = \"All\"\n",
        "    # Create the visualization\n",
        "    plt.figure(figsize=(30, 20))\n",
        "    plt.title(f'GDELT Project: {title}\\nPerson Co-occurrence Network for \"{search_person}\"', \n",
        "              y=0.97, fontsize=20, fontweight='bold')\n",
        "    \n",
        "    # Draw the network\n",
        "    pos = nx.spring_layout(g, k=3, iterations=50)  # Layout algorithm\n",
        "    nx.draw(g, pos, \n",
        "            with_labels=True, \n",
        "            node_color='lightblue',\n",
        "            node_size=500,\n",
        "            font_size=8,\n",
        "            font_weight='bold',\n",
        "            edge_color='gray',\n",
        "            alpha=0.7)\n",
        "    \n",
        "    # Add edge labels for weights (only for top edges to avoid clutter)\n",
        "    edge_labels = {}\n",
        "    for (u, v, d) in g.edges(data=True):\n",
        "        if d['weight'] > 5:  # Only show labels for significant connections\n",
        "            edge_labels[(u, v)] = f\"{d['weight']:.1f}\"\n",
        "    \n",
        "    nx.draw_networkx_edge_labels(g, pos, edge_labels, font_size=6)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print some network statistics\n",
        "    print(f\"\\nüìä Network Statistics:\")\n",
        "    print(f\"   Nodes: {g.number_of_nodes()}\")\n",
        "    print(f\"   Edges: {g.number_of_edges()}\")\n",
        "    print(f\"   Average degree: {sum(dict(g.degree()).values()) / g.number_of_nodes():.2f}\")\n",
        "    \n",
        "    # Find the most connected nodes\n",
        "    degree_centrality = nx.degree_centrality(g)\n",
        "    top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    print(f\"\\nüèÜ Most Connected Nodes:\")\n",
        "    for i, (node, centrality) in enumerate(top_nodes, 1):\n",
        "        print(f\"   {i}. {node}: {centrality:.3f}\")\n",
        "    \n",
        "    return g\n",
        "\n",
        "# Create the network graph from the co-occurrence results\n",
        "network_graph = create_person_network_graph(cooccurrence_results, search_person)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
